---
title: 'Fast Stochastic Bregman Gradient Methods: Sharp Analysis and Variance Reduction'
abstract: 'We study the problem of minimizing a relatively-smooth convex function
  using stochastic Bregman gradient methods. We first prove the convergence of Bregman
  Stochastic Gradient Descent (BSGD) to a region that depends on the noise (magnitude
  of the gradients) at the optimum. In particular, BSGD quickly converges to the exact
  minimizer when this noise is zero (interpolation setting, in which the data is fit
  perfectly). Otherwise, when the objective has a finite sum structure, we show that
  variance reduction can be used to counter the effect of noise. In particular, fast
  convergence to the exact minimizer can be obtained under additional regularity assumptions
  on the Bregman reference function. We illustrate the effectiveness of our approach
  on two key applications of relative smoothness: tomographic reconstruction with
  Poisson noise and statistical preconditioning for distributed optimization.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dragomir21a
month: 0
tex_title: 'Fast Stochastic Bregman Gradient Methods: Sharp Analysis and Variance
  Reduction'
firstpage: 2815
lastpage: 2825
page: 2815-2825
order: 2815
cycles: false
bibtex_author: Dragomir, Radu Alexandru and Even, Mathieu and Hendrikx, Hadrien
author:
- given: Radu Alexandru
  family: Dragomir
- given: Mathieu
  family: Even
- given: Hadrien
  family: Hendrikx
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/dragomir21a/dragomir21a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v139/dragomir21a/dragomir21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
