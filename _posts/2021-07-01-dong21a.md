---
title: 'Attention is not all you need: pure attention loses rank doubly exponentially
  with depth'
abstract: 'Attention-based architectures have become ubiquitous in machine learning.
  Yet, our understanding of the reasons for their effectiveness remains limited. This
  work proposes a new way to understand self-attention networks: we show that their
  output can be decomposed into a sum of smaller terms—or paths—each involving the
  operation of a sequence of attention heads across layers. Using this path decomposition,
  we prove that self-attention possesses a strong inductive bias towards "token uniformity".
  Specifically, without skip connections or multi-layer perceptrons (MLPs), the output
  converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections
  and MLPs stop the output from degeneration. Our experiments verify the convergence
  results on standard transformer architectures.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dong21a
month: 0
tex_title: 'Attention is not all you need: pure attention loses rank doubly exponentially
  with depth'
firstpage: 2793
lastpage: 2803
page: 2793-2803
order: 2793
cycles: false
bibtex_author: Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas
author:
- given: Yihe
  family: Dong
- given: Jean-Baptiste
  family: Cordonnier
- given: Andreas
  family: Loukas
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/dong21a/dong21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/dong21a/dong21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
