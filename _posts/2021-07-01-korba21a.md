---
title: Kernel Stein Discrepancy Descent
abstract: Among dissimilarities between probability distributions, the Kernel Stein
  Discrepancy (KSD) has received much interest recently. We investigate the properties
  of its Wasserstein gradient flow to approximate a target probability distribution
  $\pi$ on $\mathbb{R}^d$, known up to a normalization constant. This leads to a straightforwardly
  implementable, deterministic score-based method to sample from $\pi$, named KSD
  Descent, which uses a set of particles to approximate $\pi$. Remarkably, owing to
  a tractable loss function, KSD Descent can leverage robust parameter-free optimization
  schemes such as L-BFGS; this contrasts with other popular particle-based schemes
  such as the Stein Variational Gradient Descent algorithm. We study the convergence
  properties of KSD Descent and demonstrate its practical relevance. However, we also
  highlight failure cases by showing that the algorithm can get stuck in spurious
  local minima.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: korba21a
month: 0
tex_title: Kernel Stein Discrepancy Descent
firstpage: 5719
lastpage: 5730
page: 5719-5730
order: 5719
cycles: false
bibtex_author: Korba, Anna and Aubin-Frankowski, Pierre-Cyril and Majewski, Szymon
  and Ablin, Pierre
author:
- given: Anna
  family: Korba
- given: Pierre-Cyril
  family: Aubin-Frankowski
- given: Szymon
  family: Majewski
- given: Pierre
  family: Ablin
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/korba21a/korba21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/korba21a/korba21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
