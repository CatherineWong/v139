---
title: On the Generalization Power of Overfitted Two-Layer Neural Tangent Kernel Models
abstract: In this paper, we study the generalization performance of min $\ell_2$-norm
  overfitting solutions for the neural tangent kernel (NTK) model of a two-layer neural
  network with ReLU activation that has no bias term. We show that, depending on the
  ground-truth function, the test error of overfitted NTK models exhibits characteristics
  that are different from the "double-descent" of other overparameterized linear models
  with simple Fourier or Gaussian features. Specifically, for a class of learnable
  functions, we provide a new upper bound of the generalization error that approaches
  a small limiting value, even when the number of neurons $p$ approaches infinity.
  This limiting value further decreases with the number of training samples $n$. For
  functions outside of this class, we provide a lower bound on the generalization
  error that does not diminish to zero even when $n$ and $p$ are both large.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ju21a
month: 0
tex_title: On the Generalization Power of Overfitted Two-Layer Neural Tangent Kernel
  Models
firstpage: 5137
lastpage: 5147
page: 5137-5147
order: 5137
cycles: false
bibtex_author: Ju, Peizhong and Lin, Xiaojun and Shroff, Ness
author:
- given: Peizhong
  family: Ju
- given: Xiaojun
  family: Lin
- given: Ness
  family: Shroff
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/ju21a/ju21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/ju21a/ju21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
