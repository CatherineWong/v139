---
title: How Do Adam and Training Strategies Help BNNs Optimization
abstract: The best performing Binary Neural Networks (BNNs) are usually attained using
  Adam optimization and its multi-step training variants. However, to the best of
  our knowledge, few studies explore the fundamental reasons why Adam is superior
  to other optimizers like SGD for BNN optimization or provide analytical explanations
  that support specific training strategies. To address this, in this paper we first
  investigate the trajectories of gradients and weights in BNNs during the training
  process. We show the regularization effect of second-order momentum in Adam is crucial
  to revitalize the weights that are dead due to the activation saturation in BNNs.
  We find that Adam, through its adaptive learning rate strategy, is better equipped
  to handle the rugged loss surface of BNNs and reaches a better optimum with higher
  generalization ability. Furthermore, we inspect the intriguing role of the real-valued
  weights in binary networks, and reveal the effect of weight decay on the stability
  and sluggishness of BNN optimization. Through extensive experiments and analysis,
  we derive a simple training scheme, building on existing Adam-based optimization,
  which achieves 70.5% top-1 accuracy on the ImageNet dataset using the same architecture
  as the state-of-the-art ReActNet while achieving 1.1% higher accuracy. Code and
  models are available at https://github.com/liuzechun/AdamBNN.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu21t
month: 0
tex_title: How Do Adam and Training Strategies Help BNNs Optimization
firstpage: 6936
lastpage: 6946
page: 6936-6946
order: 6936
cycles: false
bibtex_author: Liu, Zechun and Shen, Zhiqiang and Li, Shichao and Helwegen, Koen and
  Huang, Dong and Cheng, Kwang-Ting
author:
- given: Zechun
  family: Liu
- given: Zhiqiang
  family: Shen
- given: Shichao
  family: Li
- given: Koen
  family: Helwegen
- given: Dong
  family: Huang
- given: Kwang-Ting
  family: Cheng
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/liu21t/liu21t.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
