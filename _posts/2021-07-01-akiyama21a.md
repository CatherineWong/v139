---
title: On Learnability via Gradient Method for Two-Layer ReLU Neural Networks in Teacher-Student
  Setting
abstract: Deep learning empirically achieves high performance in many applications,
  but its training dynamics has not been fully understood theoretically. In this paper,
  we explore theoretical analysis on training two-layer ReLU neural networks in a
  teacher-student regression model, in which a student network learns an unknown teacher
  network through its outputs. We show that with a specific regularization and sufficient
  over-parameterization, the student network can identify the parameters of the teacher
  network with high probability via gradient descent with a norm dependent stepsize
  even though the objective function is highly non-convex. The key theoretical tool
  is the measure representation of the neural networks and a novel application of
  a dual certificate argument for sparse estimation on a measure space. We analyze
  the global minima and global convergence property in the measure space.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: akiyama21a
month: 0
tex_title: On Learnability via Gradient Method for Two-Layer ReLU Neural Networks
  in Teacher-Student Setting
firstpage: 152
lastpage: 162
page: 152-162
order: 152
cycles: false
bibtex_author: Akiyama, Shunta and Suzuki, Taiji
author:
- given: Shunta
  family: Akiyama
- given: Taiji
  family: Suzuki
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/akiyama21a/akiyama21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/akiyama21a/akiyama21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
