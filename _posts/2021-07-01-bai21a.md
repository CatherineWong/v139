---
title: How Important is the Train-Validation Split in Meta-Learning?
abstract: Meta-learning aims to perform fast adaptation on a new task through learning
  a “prior” from multiple existing tasks. A common practice in meta-learning is to
  perform a train-validation split (\emph{train-val method}) where the prior adapts
  to the task on one split of the data, and the resulting predictor is evaluated on
  another split. Despite its prevalence, the importance of the train-validation split
  is not well understood either in theory or in practice, particularly in comparison
  to the more direct \emph{train-train method}, which uses all the per-task data for
  both training and evaluation. We provide a detailed theoretical study on whether
  and when the train-validation split is helpful in the linear centroid meta-learning
  problem. In the agnostic case, we show that the expected loss of the train-val method
  is minimized at the optimal prior for meta testing, and this is not the case for
  the train-train method in general without structural assumptions on the data. In
  contrast, in the realizable case where the data are generated from linear models,
  we show that both the train-val and train-train losses are minimized at the optimal
  prior in expectation. Further, perhaps surprisingly, our main result shows that
  the train-train method achieves a \emph{strictly better} excess loss in this realizable
  case, even when the regularization parameter and split ratio are optimally tuned
  for both methods. Our results highlight that sample splitting may not always be
  preferable, especially when the data is realizable by the model. We validate our
  theories by experimentally showing that the train-train method can indeed outperform
  the train-val method, on both simulations and real meta-learning tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bai21a
month: 0
tex_title: How Important is the Train-Validation Split in Meta-Learning?
firstpage: 543
lastpage: 553
page: 543-553
order: 543
cycles: false
bibtex_author: Bai, Yu and Chen, Minshuo and Zhou, Pan and Zhao, Tuo and Lee, Jason
  and Kakade, Sham and Wang, Huan and Xiong, Caiming
author:
- given: Yu
  family: Bai
- given: Minshuo
  family: Chen
- given: Pan
  family: Zhou
- given: Tuo
  family: Zhao
- given: Jason
  family: Lee
- given: Sham
  family: Kakade
- given: Huan
  family: Wang
- given: Caiming
  family: Xiong
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/bai21a/bai21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/bai21a/bai21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
