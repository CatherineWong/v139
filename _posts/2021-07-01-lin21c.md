---
title: 'Quasi-global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous
  Data'
abstract: Decentralized training of deep learning models is a key element for enabling
  data privacy and on-device learning over networks. In realistic learning scenarios,
  the presence of heterogeneity across different clients’ local datasets poses an
  optimization challenge and may severely deteriorate the generalization performance.
  In this paper, we investigate and identify the limitation of several decentralized
  optimization algorithms for different degrees of data heterogeneity. We propose
  a novel momentum-based method to mitigate this decentralized training difficulty.
  We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10,
  ImageNet, and AG News) and several network topologies (Ring and Social Network)
  that our method is much more robust to the heterogeneity of clients’ data than other
  existing methods, by a significant improvement in test performance (1%-20%).
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lin21c
month: 0
tex_title: 'Quasi-global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous
  Data'
firstpage: 6654
lastpage: 6665
page: 6654-6665
order: 6654
cycles: false
bibtex_author: Lin, Tao and Karimireddy, Sai Praneeth and Stich, Sebastian and Jaggi,
  Martin
author:
- given: Tao
  family: Lin
- given: Sai Praneeth
  family: Karimireddy
- given: Sebastian
  family: Stich
- given: Martin
  family: Jaggi
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/lin21c/lin21c.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/lin21c/lin21c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
