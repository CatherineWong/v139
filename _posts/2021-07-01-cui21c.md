---
title: 'GBHT: Gradient Boosting Histogram Transform for Density Estimation'
abstract: In this paper, we propose a density estimation algorithm called \textit{Gradient
  Boosting Histogram Transform} (GBHT), where we adopt the \textit{Negative Log Likelihood}
  as the loss function to make the boosting procedure available for the unsupervised
  tasks. From a learning theory viewpoint, we first prove fast convergence rates for
  GBHT with the smoothness assumption that the underlying density function lies in
  the space $C^{0,\alpha}$. Then when the target density function lies in spaces $C^{1,\alpha}$,
  we present an upper bound for GBHT which is smaller than the lower bound of its
  corresponding base learner, in the sense of convergence rates. To the best of our
  knowledge, we make the first attempt to theoretically explain why boosting can enhance
  the performance of its base learners for density estimation problems. In experiments,
  we not only conduct performance comparisons with the widely used KDE, but also apply
  GBHT to anomaly detection to showcase a further application of GBHT.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cui21c
month: 0
tex_title: 'GBHT: Gradient Boosting Histogram Transform for Density Estimation'
firstpage: 2233
lastpage: 2243
page: 2233-2243
order: 2233
cycles: false
bibtex_author: Cui, Jingyi and Hang, Hanyuan and Wang, Yisen and Lin, Zhouchen
author:
- given: Jingyi
  family: Cui
- given: Hanyuan
  family: Hang
- given: Yisen
  family: Wang
- given: Zhouchen
  family: Lin
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/cui21c/cui21c.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/cui21c/cui21c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
