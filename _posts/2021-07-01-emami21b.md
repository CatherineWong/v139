---
title: Implicit Bias of Linear RNNs
abstract: Contemporary wisdom based on empirical studies suggests that standard recurrent
  neural networks (RNNs) do not perform well on tasks requiring long-term memory.
  However, RNNsâ€™ poor ability to capture long-term dependencies has not been fully
  understood. This paper provides a rigorous explanation of this property in the special
  case of linear RNNs. Although this work is limited to linear RNNs, even these systems
  have traditionally been difficult to analyze due to their non-linear parameterization.
  Using recently-developed kernel regime analysis, our main result shows that as the
  number of hidden units goes to infinity, linear RNNs learned from random initializations
  are functionally equivalent to a certain weighted 1D-convolutional network. Importantly,
  the weightings in the equivalent model cause an implicit bias to elements with smaller
  time lags in the convolution, and hence shorter memory. The degree of this bias
  depends on the variance of the transition matrix at initialization and is related
  to the classic exploding and vanishing gradients problem. The theory is validated
  with both synthetic and real data experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: emami21b
month: 0
tex_title: Implicit Bias of Linear RNNs
firstpage: 2982
lastpage: 2992
page: 2982-2992
order: 2982
cycles: false
bibtex_author: Emami, Melikasadat and Sahraee-Ardakan, Mojtaba and Pandit, Parthe
  and Rangan, Sundeep and Fletcher, Alyson K
author:
- given: Melikasadat
  family: Emami
- given: Mojtaba
  family: Sahraee-Ardakan
- given: Parthe
  family: Pandit
- given: Sundeep
  family: Rangan
- given: Alyson K
  family: Fletcher
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/emami21b/emami21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/emami21b/emami21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
