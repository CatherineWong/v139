---
title: Noise and Fluctuation of Finite Learning Rate Stochastic Gradient Descent
abstract: In the vanishing learning rate regime, stochastic gradient descent (SGD)
  is now relatively well understood. In this work, we propose to study the basic properties
  of SGD and its variants in the non-vanishing learning rate regime. The focus is
  on deriving exactly solvable results and discussing their implications. The main
  contributions of this work are to derive the stationary distribution for discrete-time
  SGD in a quadratic loss function with and without momentum; in particular, one implication
  of our result is that the fluctuation caused by discrete-time dynamics takes a distorted
  shape and is dramatically larger than a continuous-time theory could predict. Examples
  of applications of the proposed theory considered in this work include the approximation
  error of variants of SGD, the effect of minibatch noise, the optimal Bayesian inference,
  the escape rate from a sharp minimum, and the stationary covariance of a few second-order
  methods including damped Newtonâ€™s method, natural gradient descent, and Adam.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu21ad
month: 0
tex_title: Noise and Fluctuation of Finite Learning Rate Stochastic Gradient Descent
firstpage: 7045
lastpage: 7056
page: 7045-7056
order: 7045
cycles: false
bibtex_author: Liu, Kangqiao and Ziyin, Liu and Ueda, Masahito
author:
- given: Kangqiao
  family: Liu
- given: Liu
  family: Ziyin
- given: Masahito
  family: Ueda
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/liu21ad/liu21ad.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/liu21ad/liu21ad-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
