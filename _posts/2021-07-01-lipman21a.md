---
title: Phase Transitions, Distance Functions, and Implicit Neural Representations
abstract: Representing surfaces as zero level sets of neural networks recently emerged
  as a powerful modeling paradigm, named Implicit Neural Representations (INRs), serving
  numerous downstream applications in geometric deep learning and 3D vision. Training
  INRs previously required choosing between occupancy and distance function representation
  and different losses with unknown limit behavior and/or bias. In this paper we draw
  inspiration from the theory of phase transitions of fluids and suggest a loss for
  training INRs that learns a density function that converges to a proper occupancy
  function, while its log transform converges to a distance function. Furthermore,
  we analyze the limit minimizer of this loss showing it satisfies the reconstruction
  constraints and has minimal surface perimeter, a desirable inductive bias for surface
  reconstruction. Training INRs with this new loss leads to state-of-the-art reconstructions
  on a standard benchmark.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lipman21a
month: 0
tex_title: Phase Transitions, Distance Functions, and Implicit Neural Representations
firstpage: 6702
lastpage: 6712
page: 6702-6712
order: 6702
cycles: false
bibtex_author: Lipman, Yaron
author:
- given: Yaron
  family: Lipman
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/lipman21a/lipman21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/lipman21a/lipman21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
