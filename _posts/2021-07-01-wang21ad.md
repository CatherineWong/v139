---
title: 'Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training
  and Effective Adaptation'
abstract: Multi-task learning (MTL) aims to improve the generalization of several
  related tasks by learning them jointly. As a comparison, in addition to the joint
  training scheme, modern meta-learning allows unseen tasks with limited labels during
  the test phase, in the hope of fast adaptation over them. Despite the subtle difference
  between MTL and meta-learning in the problem formulation, both learning paradigms
  share the same insight that the shared structure between existing training tasks
  could lead to better generalization and adaptation. In this paper, we take one important
  step further to understand the close connection between these two learning paradigms,
  through both theoretical analysis and empirical investigation. Theoretically, we
  first demonstrate that MTL shares the same optimization formulation with a class
  of gradient-based meta-learning (GBML) algorithms. We then prove that for over-parameterized
  neural networks with sufficient depth, the learned predictive functions of MTL and
  GBML are close. In particular, this result implies that the predictions given by
  these two models are similar over the same unseen task. Empirically, we corroborate
  our theoretical findings by showing that, with proper implementation, MTL is competitive
  against state-of-the-art GBML algorithms on a set of few-shot image classification
  benchmarks. Since existing GBML algorithms often involve costly second-order bi-level
  optimization, our first-order MTL method is an order of magnitude faster on large-scale
  datasets such as mini-ImageNet. We believe this work could help bridge the gap between
  these two learning paradigms, and provide a computationally efficient alternative
  to GBML that also supports fast task adaptation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang21ad
month: 0
tex_title: 'Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training
  and Effective Adaptation'
firstpage: 10991
lastpage: 11002
page: 10991-11002
order: 10991
cycles: false
bibtex_author: Wang, Haoxiang and Zhao, Han and Li, Bo
author:
- given: Haoxiang
  family: Wang
- given: Han
  family: Zhao
- given: Bo
  family: Li
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/wang21ad/wang21ad.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/wang21ad/wang21ad-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
