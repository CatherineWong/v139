---
title: 'UnICORNN: A recurrent model for learning very long time dependencies'
abstract: The design of recurrent neural networks (RNNs) to accurately process sequential
  inputs with long-time dependencies is very challenging on account of the exploding
  and vanishing gradient problem. To overcome this, we propose a novel RNN architecture
  which is based on a structure preserving discretization of a Hamiltonian system
  of second-order ordinary differential equations that models networks of oscillators.
  The resulting RNN is fast, invertible (in time), memory efficient and we derive
  rigorous bounds on the hidden state gradients to prove the mitigation of the exploding
  and vanishing gradient problem. A suite of experiments are presented to demonstrate
  that the proposed RNN provides state of the art performance on a variety of learning
  tasks with (very) long-time dependencies.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rusch21a
month: 0
tex_title: 'UnICORNN: A recurrent model for learning very long time dependencies'
firstpage: 9168
lastpage: 9178
page: 9168-9178
order: 9168
cycles: false
bibtex_author: Rusch, T. Konstantin and Mishra, Siddhartha
author:
- given: T. Konstantin
  family: Rusch
- given: Siddhartha
  family: Mishra
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/rusch21a/rusch21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/rusch21a/rusch21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
