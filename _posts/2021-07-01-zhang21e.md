---
title: Near Optimal Reward-Free Reinforcement Learning
abstract: 'We study the reward-free reinforcement learning framework, which is particularly
  suitable for batch reinforcement learning and scenarios where one needs policies
  for multiple reward functions. This framework has two phases: in the exploration
  phase, the agent collects trajectories by interacting with the environment without
  using any reward signal; in the planning phase, the agent needs to return a near-optimal
  policy for arbitrary reward functions. %This framework is suitable for batch RL
  setting and the setting where there are multiple reward functions of interes We
  give a new efficient algorithm, \textbf{S}taged \textbf{S}ampling + \textbf{T}runcated
  \textbf{P}lanning (\algoname), which interacts with the environment at most $O\left(
  \frac{S^2A}{\epsilon^2}\poly\log\left(\frac{SAH}{\epsilon}\right) \right)$ episodes
  in the exploration phase, and guarantees to output a near-optimal policy for arbitrary
  reward functions in the planning phase, where $S$ is the size of state space, $A$
  is the size of action space, $H$ is the planning horizon, and $\epsilon$ is the
  target accuracy relative to the total reward. Notably, our sample complexity scales
  only \emph{logarithmically} with $H$, in contrast to all existing results which
  scale \emph{polynomially} with $H$. Furthermore, this bound matches the minimax
  lower bound $\Omega\left(\frac{S^2A}{\epsilon^2}\right)$ up to logarithmic factors.
  Our results rely on three new techniques : 1) A new sufficient condition for the
  dataset to plan for an $\epsilon$-suboptimal policy % for any totally bounded reward
  function ; 2) A new way to plan efficiently under the proposed condition using soft-truncated
  planning; 3) Constructing extended MDP to maximize the truncated accumulative rewards
  efficiently.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21e
month: 0
tex_title: Near Optimal Reward-Free Reinforcement Learning
firstpage: 12402
lastpage: 12412
page: 12402-12412
order: 12402
cycles: false
bibtex_author: Zhang, Zihan and Du, Simon and Ji, Xiangyang
author:
- given: Zihan
  family: Zhang
- given: Simon
  family: Du
- given: Xiangyang
  family: Ji
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zhang21e/zhang21e.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/zhang21e/zhang21e-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
