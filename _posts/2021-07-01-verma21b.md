---
title: Sparsifying Networks via Subdifferential Inclusion
abstract: 'Sparsifying deep neural networks is of paramount interest in many areas,
  especially when those networks have to be implemented on low-memory devices. In
  this article, we propose a new formulation of the problem of generating sparse weights
  for a pre-trained neural network. By leveraging the properties of standard nonlinear
  activation functions, we show that the problem is equivalent to an approximate subdifferential
  inclusion problem. The accuracy of the approximation controls the sparsity. We show
  that the proposed approach is valid for a broad class of activation functions (ReLU,
  sigmoid, softmax). We propose an iterative optimization algorithm to induce sparsity
  whose convergence is guaranteed. Because of the algorithm flexibility, the sparsity
  can be ensured from partial training data in a minibatch manner. To demonstrate
  the effectiveness of our method, we perform experiments on various networks in different
  applicative contexts: image classification, speech recognition, natural language
  processing, and time-series forecasting.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: verma21b
month: 0
tex_title: Sparsifying Networks via Subdifferential Inclusion
firstpage: 10542
lastpage: 10552
page: 10542-10552
order: 10542
cycles: false
bibtex_author: Verma, Sagar and Pesquet, Jean-Christophe
author:
- given: Sagar
  family: Verma
- given: Jean-Christophe
  family: Pesquet
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/verma21b/verma21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/verma21b/verma21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
