---
title: Deep Coherent Exploration for Continuous Control
abstract: In policy search methods for reinforcement learning (RL), exploration is
  often performed by injecting noise either in action space at each step independently
  or in parameter space over each full trajectory. In prior work, it has been shown
  that with linear policies, a more balanced trade-off between these two exploration
  strategies is beneficial. However, that method did not scale to policies using deep
  neural networks. In this paper, we introduce deep coherent exploration, a general
  and scalable exploration framework for deep RL algorithms for continuous control,
  that generalizes step-based and trajectory-based exploration. This framework models
  the last layer parameters of the policy network as latent variables and uses a recursive
  inference step within the policy update to handle these latent variables in a scalable
  manner. We find that deep coherent exploration improves the speed and stability
  of learning of A2C, PPO, and SAC on several continuous control tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21t
month: 0
tex_title: Deep Coherent Exploration for Continuous Control
firstpage: 12567
lastpage: 12577
page: 12567-12577
order: 12567
cycles: false
bibtex_author: Zhang, Yijie and Van Hoof, Herke
author:
- given: Yijie
  family: Zhang
- given: Herke
  family: Van Hoof
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zhang21t/zhang21t.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/zhang21t/zhang21t-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
