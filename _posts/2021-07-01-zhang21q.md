---
title: On-Policy Deep Reinforcement Learning for the Average-Reward Criterion
abstract: We develop theory and algorithms for average-reward on-policy Reinforcement
  Learning (RL). We first consider bounding the difference of the long-term average
  reward for two policies. We show that previous work based on the discounted return
  (Schulman et al. 2015, Achiam et al. 2017) results in a non-meaningful lower bound
  in the average reward setting. By addressing the average-reward criterion directly,
  we then derive a novel bound which depends on the average divergence between the
  policies and on Kemenyâ€™s constant. Based on this bound, we develop an iterative
  procedure which produces a sequence of monotonically improved policies for the average
  reward criterion. This iterative procedure can then be combined with classic Deep
  Reinforcement Learning (DRL) methods, resulting in practical DRL algorithms that
  target the long-run average reward criterion. In particular, we demonstrate that
  Average-Reward TRPO (ATRPO), which adapts the on-policy TRPO algorithm to the average-reward
  criterion, significantly outperforms TRPO in the most challenging MuJuCo environments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21q
month: 0
tex_title: On-Policy Deep Reinforcement Learning for the Average-Reward Criterion
firstpage: 12535
lastpage: 12545
page: 12535-12545
order: 12535
cycles: false
bibtex_author: Zhang, Yiming and Ross, Keith W
author:
- given: Yiming
  family: Zhang
- given: Keith W
  family: Ross
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zhang21q/zhang21q.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/zhang21q/zhang21q-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
