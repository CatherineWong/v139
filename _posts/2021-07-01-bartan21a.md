---
title: Training Quantized Neural Networks to Global Optimality via Semidefinite Programming
abstract: Neural networks (NNs) have been extremely successful across many tasks in
  machine learning. Quantization of NN weights has become an important topic due to
  its impact on their energy efficiency, inference time and deployment on hardware.
  Although post-training quantization is well-studied, training optimal quantized
  NNs involves combinatorial non-convex optimization problems which appear intractable.
  In this work, we introduce a convex optimization strategy to train quantized NNs
  with polynomial activations. Our method leverages hidden convexity in two-layer
  neural networks from the recent literature, semidefinite lifting, and Grothendieckâ€™s
  identity. Surprisingly, we show that certain quantized NN problems can be solved
  to global optimality provably in polynomial time in all relevant parameters via
  tight semidefinite relaxations. We present numerical examples to illustrate the
  effectiveness of our method.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bartan21a
month: 0
tex_title: Training Quantized Neural Networks to Global Optimality via Semidefinite
  Programming
firstpage: 694
lastpage: 704
page: 694-704
order: 694
cycles: false
bibtex_author: Bartan, Burak and Pilanci, Mert
author:
- given: Burak
  family: Bartan
- given: Mert
  family: Pilanci
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/bartan21a/bartan21a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v139/bartan21a/bartan21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
