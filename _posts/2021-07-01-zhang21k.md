---
title: Learning from Noisy Labels with No Change to the Training Process
abstract: 'There has been much interest in recent years in developing learning algorithms
  that can learn accurate classifiers from data with noisy labels. A widely-studied
  noise model is that of \emph{class-conditional noise} (CCN), wherein a label $y$
  is flipped to a label $\tilde{y}$ with some associated noise probability that depends
  on both $y$ and $\tilde{y}$. In the multiclass setting, all previously proposed
  algorithms under the CCN model involve changing the training process, by introducing
  a ‘noise-correction’ to the surrogate loss to be minimized over the noisy training
  examples. In this paper, we show that this is really unnecessary: one can simply
  perform class probability estimation (CPE) on the noisy examples, e.g. using a standard
  (multiclass) logistic regression algorithm, and then apply noise-correction only
  in the final prediction step. This means that the training algorithm itself does
  not need any change, and one can simply use standard off-the-shelf implementations
  with no modification to the code for training. Our approach can handle general multiclass
  loss matrices, including the usual 0-1 loss but also other losses such as those
  used for ordinal regression problems. We also provide a quantitative regret transfer
  bound, which bounds the target regret on the true distribution in terms of the CPE
  regret on the noisy distribution; in doing so, we extend the notion of strong properness
  introduced for binary losses by Agarwal (2014) to the multiclass case. Our bound
  suggests that the sample complexity of learning under CCN increases as the noise
  matrix approaches singularity. We also provide fixes and potential improvements
  for noise estimation methods that involve computing anchor points. Our experiments
  confirm our theoretical findings.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21k
month: 0
tex_title: Learning from Noisy Labels with No Change to the Training Process
firstpage: 12468
lastpage: 12478
page: 12468-12478
order: 12468
cycles: false
bibtex_author: Zhang, Mingyuan and Lee, Jane and Agarwal, Shivani
author:
- given: Mingyuan
  family: Zhang
- given: Jane
  family: Lee
- given: Shivani
  family: Agarwal
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zhang21k/zhang21k.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/zhang21k/zhang21k-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
