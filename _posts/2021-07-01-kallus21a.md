---
title: Optimal Off-Policy Evaluation from Multiple Logging Policies
abstract: We study off-policy evaluation (OPE) from multiple logging policies, each
  generating a dataset of fixed size, i.e., stratified sampling. Previous work noted
  that in this setting the ordering of the variances of different importance sampling
  estimators is instance-dependent, which brings up a dilemma as to which importance
  sampling weights to use. In this paper, we resolve this dilemma by finding the OPE
  estimator for multiple loggers with minimum variance for any instance, i.e., the
  efficient one. In particular, we establish the efficiency bound under stratified
  sampling and propose an estimator achieving this bound when given consistent $q$-estimates.
  To guard against misspecification of $q$-functions, we also provide a way to choose
  the control variate in a hypothesis class to minimize variance. Extensive experiments
  demonstrate the benefits of our methodsâ€™ efficiently leveraging of the stratified
  sampling of off-policy data from multiple loggers.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kallus21a
month: 0
tex_title: Optimal Off-Policy Evaluation from Multiple Logging Policies
firstpage: 5247
lastpage: 5256
page: 5247-5256
order: 5247
cycles: false
bibtex_author: Kallus, Nathan and Saito, Yuta and Uehara, Masatoshi
author:
- given: Nathan
  family: Kallus
- given: Yuta
  family: Saito
- given: Masatoshi
  family: Uehara
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/kallus21a/kallus21a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v139/kallus21a/kallus21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
