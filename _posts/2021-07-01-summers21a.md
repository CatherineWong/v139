---
title: Nondeterminism and Instability in Neural Network Optimization
abstract: Nondeterminism in neural network optimization produces uncertainty in performance,
  making small improvements difficult to discern from run-to-run variability. While
  uncertainty can be reduced by training multiple model copies, doing so is time-consuming,
  costly, and harms reproducibility. In this work, we establish an experimental protocol
  for understanding the effect of optimization nondeterminism on model diversity,
  allowing us to isolate the effects of a variety of sources of nondeterminism. Surprisingly,
  we find that all sources of nondeterminism have similar effects on measures of model
  diversity. To explain this intriguing fact, we identify the instability of model
  training, taken as an end-to-end procedure, as the key determinant. We show that
  even one-bit changes in initial parameters result in models converging to vastly
  different values. Last, we propose two approaches for reducing the effects of instability
  on run-to-run variability.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: summers21a
month: 0
tex_title: Nondeterminism and Instability in Neural Network Optimization
firstpage: 9913
lastpage: 9922
page: 9913-9922
order: 9913
cycles: false
bibtex_author: Summers, Cecilia and Dinneen, Michael J.
author:
- given: Cecilia
  family: Summers
- given: Michael J.
  family: Dinneen
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/summers21a/summers21a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v139/summers21a/summers21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
