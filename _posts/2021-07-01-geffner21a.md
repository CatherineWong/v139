---
title: On the difficulty of unbiased alpha divergence minimization
abstract: Several approximate inference algorithms have been proposed to minimize
  an alpha-divergence between an approximating distribution and a target distribution.
  Many of these algorithms introduce bias, the magnitude of which becomes problematic
  in high dimensions. Other algorithms are unbiased. These often seem to suffer from
  high variance, but little is rigorously known. In this work we study unbiased methods
  for alpha-divergence minimization through the Signal-to-Noise Ratio (SNR) of the
  gradient estimator. We study several representative scenarios where strong analytical
  results are possible, such as fully-factorized or Gaussian distributions. We find
  that when alpha is not zero, the SNR worsens exponentially in the dimensionality
  of the problem. This casts doubt on the practicality of these methods. We empirically
  confirm these theoretical results.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: geffner21a
month: 0
tex_title: On the difficulty of unbiased alpha divergence minimization
firstpage: 3650
lastpage: 3659
page: 3650-3659
order: 3650
cycles: false
bibtex_author: Geffner, Tomas and Domke, Justin
author:
- given: Tomas
  family: Geffner
- given: Justin
  family: Domke
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/geffner21a/geffner21a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v139/geffner21a/geffner21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
