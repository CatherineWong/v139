---
title: 'ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed
  Training'
abstract: The increasing size of neural network models has been critical for improvements
  in their accuracy, but device memory is not growing at the same rate. This creates
  fundamental challenges for training neural networks within limited memory environments.
  In this work, we propose ActNN, a memory-efficient training framework that stores
  randomly quantized activations for back propagation. We prove the convergence of
  ActNN for general network architectures, and we characterize the impact of quantization
  on the convergence via an exact expression for the gradient variance. Using our
  theory, we propose novel mixed-precision quantization strategies that exploit the
  activationâ€™s heterogeneity across feature dimensions, samples, and layers. These
  techniques can be readily applied to existing dynamic graph frameworks, such as
  PyTorch, simply by substituting the layers. We evaluate ActNN on mainstream computer
  vision models for classification, detection, and segmentation tasks. On all these
  tasks, ActNN compresses the activation to 2 bits on average, with negligible accuracy
  loss. ActNN reduces the memory footprint of the activation by 12x, and it enables
  training with a 6.6x to 14x larger batch size.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen21z
month: 0
tex_title: 'ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed
  Training'
firstpage: 1803
lastpage: 1813
page: 1803-1813
order: 1803
cycles: false
bibtex_author: Chen, Jianfei and Zheng, Lianmin and Yao, Zhewei and Wang, Dequan and
  Stoica, Ion and Mahoney, Michael and Gonzalez, Joseph
author:
- given: Jianfei
  family: Chen
- given: Lianmin
  family: Zheng
- given: Zhewei
  family: Yao
- given: Dequan
  family: Wang
- given: Ion
  family: Stoica
- given: Michael
  family: Mahoney
- given: Joseph
  family: Gonzalez
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/chen21z/chen21z.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/chen21z/chen21z-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
