---
title: 'A Modular Analysis of Provable Acceleration via Polyak’s Momentum: Training
  a Wide ReLU Network and a Deep Linear Network'
abstract: Incorporating a so-called “momentum” dynamic in gradient descent methods
  is widely used in neural net training as it has been broadly observed that, at least
  empirically, it often leads to significantly faster convergence. At the same time,
  there are very few theoretical guarantees in the literature to explain this apparent
  acceleration effect. Even for the classical strongly convex quadratic problems,
  several existing results only show Polyak’s momentum has an accelerated linear rate
  asymptotically. In this paper, we first revisit the quadratic problems and show
  a non-asymptotic accelerated linear rate of Polyak’s momentum. Then, we provably
  show that Polyak’s momentum achieves acceleration for training a one-layer wide
  ReLU network and a deep linear network, which are perhaps the two most popular canonical
  models for studying optimization and deep learning in the literature. Prior works
  (Du et al. 2019) and (Wu et al. 2019) showed that using vanilla gradient descent,
  and with an use of over-parameterization, the error decays as $(1- \Theta(\frac{1}{
  \kappa’}))^t$ after $t$ iterations, where $\kappa’$ is the condition number of a
  Gram Matrix. Our result shows that with the appropriate choice of parameters Polyak’s
  momentum has a rate of $(1-\Theta(\frac{1}{\sqrt{\kappa’}}))^t$. For the deep linear
  network, prior work (Hu et al. 2020) showed that vanilla gradient descent has a
  rate of $(1-\Theta(\frac{1}{\kappa}))^t$, where $\kappa$ is the condition number
  of a data matrix. Our result shows an acceleration rate $(1- \Theta(\frac{1}{\sqrt{\kappa}}))^t$
  is achievable by Polyak’s momentum. This work establishes that momentum does indeed
  speed up neural net training.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang21n
month: 0
tex_title: 'A Modular Analysis of Provable Acceleration via Polyak’s Momentum: Training
  a Wide ReLU Network and a Deep Linear Network'
firstpage: 10816
lastpage: 10827
page: 10816-10827
order: 10816
cycles: false
bibtex_author: Wang, Jun-Kun and Lin, Chi-Heng and Abernethy, Jacob D
author:
- given: Jun-Kun
  family: Wang
- given: Chi-Heng
  family: Lin
- given: Jacob D
  family: Abernethy
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/wang21n/wang21n.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/wang21n/wang21n-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
