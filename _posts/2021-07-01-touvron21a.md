---
title: Training data-efficient image transformers & distillation through attention
abstract: Recently, neural networks purely based on attention were shown to address
  image understanding tasks such as image classification. These high-performing vision
  transformers are pre-trained with hundreds of millions of images using a large infrastructure,
  thereby limiting their adoption. In this work, we produce competitive convolution-free
  transformers trained on ImageNet only using a single computer in less than 3 days.
  Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1%
  (single-crop) on ImageNet with no external data. We also introduce a teacher-student
  strategy specific to transformers. It relies on a distillation token ensuring that
  the student learns from the teacher through attention, typically from a convnet
  teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state
  of the art on ImageNet, and similarly when transferred to other tasks. We will share
  our code and models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: touvron21a
month: 0
tex_title: Training data-efficient image transformers & distillation through attention
firstpage: 10347
lastpage: 10357
page: 10347-10357
order: 10347
cycles: false
bibtex_author: Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco
  and Sablayrolles, Alexandre and Jegou, Herve
author:
- given: Hugo
  family: Touvron
- given: Matthieu
  family: Cord
- given: Matthijs
  family: Douze
- given: Francisco
  family: Massa
- given: Alexandre
  family: Sablayrolles
- given: Herve
  family: Jegou
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/touvron21a/touvron21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
