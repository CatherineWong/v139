---
title: The Logical Options Framework
abstract: Learning composable policies for environments with complex rules and tasks
  is a challenging problem. We introduce a hierarchical reinforcement learning framework
  called the Logical Options Framework (LOF) that learns policies that are satisfying,
  optimal, and composable. LOF efficiently learns policies that satisfy tasks by representing
  the task as an automaton and integrating it into learning and planning. We provide
  and prove conditions under which LOF will learn satisfying, optimal policies. And
  lastly, we show how LOFâ€™s learned policies can be composed to satisfy unseen tasks
  with only 10-50 retraining steps on our benchmarks. We evaluate LOF on four tasks
  in discrete and continuous domains, including a 3D pick-and-place environment.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: araki21a
month: 0
tex_title: The Logical Options Framework
firstpage: 307
lastpage: 317
page: 307-317
order: 307
cycles: false
bibtex_author: Araki, Brandon and Li, Xiao and Vodrahalli, Kiran and Decastro, Jonathan
  and Fry, Micah and Rus, Daniela
author:
- given: Brandon
  family: Araki
- given: Xiao
  family: Li
- given: Kiran
  family: Vodrahalli
- given: Jonathan
  family: Decastro
- given: Micah
  family: Fry
- given: Daniela
  family: Rus
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/araki21a/araki21a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v139/araki21a/araki21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
