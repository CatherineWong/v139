---
title: Learning and Planning in Average-Reward Markov Decision Processes
abstract: We introduce learning and planning algorithms for average-reward MDPs, including
  1) the first general proven-convergent off-policy model-free control algorithm without
  reference states, 2) the first proven-convergent off-policy model-free prediction
  algorithm, and 3) the first off-policy learning algorithm that converges to the
  actual value function rather than to the value function plus an offset. All of our
  algorithms are based on using the temporal-difference error rather than the conventional
  error when updating the estimate of the average reward. Our proof techniques are
  a slight generalization of those by Abounadi, Bertsekas, and Borkar (2001). In experiments
  with an Access-Control Queuing Task, we show some of the difficulties that can arise
  when using methods that rely on reference states and argue that our new algorithms
  are significantly easier to use.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wan21a
month: 0
tex_title: Learning and Planning in Average-Reward Markov Decision Processes
firstpage: 10653
lastpage: 10662
page: 10653-10662
order: 10653
cycles: false
bibtex_author: Wan, Yi and Naik, Abhishek and Sutton, Richard S
author:
- given: Yi
  family: Wan
- given: Abhishek
  family: Naik
- given: Richard S
  family: Sutton
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/wan21a/wan21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/wan21a/wan21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
