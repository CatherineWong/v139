---
title: 'How to Learn when Data Reacts to Your Model: Performative Gradient Descent'
abstract: Performative distribution shift captures the setting where the choice of
  which ML model is deployed changes the data distribution. For example, a bank which
  uses the number of open credit lines to determine a customerâ€™s risk of default on
  a loan may induce customers to open more credit lines in order to improve their
  chances of being approved. Because of the interactions between the model and data
  distribution, finding the optimal model parameters is challenging. Works in this
  area have focused on finding stable points, which can be far from optimal. Here
  we introduce \emph{performative gradient descent} (PerfGD), an algorithm for computing
  performatively optimal points. Under regularity assumptions on the performative
  loss, PerfGD is the first algorithm which provably converges to an optimal point.
  PerfGD explicitly captures how changes in the model affects the data distribution
  and is simple to use. We support our findings with theory and experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: izzo21a
month: 0
tex_title: 'How to Learn when Data Reacts to Your Model: Performative Gradient Descent'
firstpage: 4641
lastpage: 4650
page: 4641-4650
order: 4641
cycles: false
bibtex_author: Izzo, Zachary and Ying, Lexing and Zou, James
author:
- given: Zachary
  family: Izzo
- given: Lexing
  family: Ying
- given: James
  family: Zou
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/izzo21a/izzo21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/izzo21a/izzo21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
