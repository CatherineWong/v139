---
title: Fast margin maximization via dual acceleration
abstract: We present and analyze a momentum-based gradient method for training linear
  classifiers with an exponentially-tailed loss (e.g., the exponential or logistic
  loss), which maximizes the classification margin on separable data at a rate of
  O(1/t^2). This contrasts with a rate of O(1/log(t)) for standard gradient descent,
  and O(1/t) for normalized gradient descent. The momentum-based method is derived
  via the convex dual of the maximum-margin problem, and specifically by applying
  Nesterov acceleration to this dual, which manages to result in a simple and intuitive
  method in the primal. This dual view can also be used to derive a stochastic variant,
  which performs adaptive non-uniform sampling via the dual variables.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ji21a
month: 0
tex_title: Fast margin maximization via dual acceleration
firstpage: 4860
lastpage: 4869
page: 4860-4869
order: 4860
cycles: false
bibtex_author: Ji, Ziwei and Srebro, Nathan and Telgarsky, Matus
author:
- given: Ziwei
  family: Ji
- given: Nathan
  family: Srebro
- given: Matus
  family: Telgarsky
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/ji21a/ji21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/ji21a/ji21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
