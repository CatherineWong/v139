---
title: Dueling Convex Optimization
abstract: We address the problem of convex optimization with preference (dueling)
  feedback. Like the traditional optimization objective, the goal is to find the optimal
  point with the least possible query complexity, however, without the luxury of even
  a zeroth order feedback. Instead, the learner can only observe a single noisy bit
  which is win-loss feedback for a pair of queried points based on their function
  values. % The problem is certainly of great practical relevance as in many real-world
  scenarios, such as recommender systems or learning from customer preferences, where
  the system feedback is often restricted to just one binary-bit preference information.
  % We consider the problem of online convex optimization (OCO) solely by actively
  querying $\{0,1\}$ noisy-comparison feedback of decision point pairs, with the objective
  of finding a near-optimal point (function minimizer) with the least possible number
  of queries. %a very general class of monotonic, non-decreasing transfer functions,
  and analyze the problem for any $d$-dimensional smooth convex function. % For the
  non-stationary OCO setup, where the underlying convex function may change over time,
  we prove an impossibility result towards achieving the above objective. We next
  focus only on the stationary OCO problem, and our main contribution lies in designing
  a normalized gradient descent based algorithm towards finding a $\epsilon$-best
  optimal point. Towards this, our algorithm is shown to yield a convergence rate
  of $\tilde O(\nicefrac{d\beta}{\epsilon \nu^2})$ ($\nu$ being the noise parameter)
  when the underlying function is $\beta$-smooth. Further we show an improved convergence
  rate of just $\tilde O(\nicefrac{d\beta}{\alpha \nu^2} \log \frac{1}{\epsilon})$
  when the function is additionally also $\alpha$-strongly convex.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: saha21b
month: 0
tex_title: Dueling Convex Optimization
firstpage: 9245
lastpage: 9254
page: 9245-9254
order: 9245
cycles: false
bibtex_author: Saha, Aadirupa and Koren, Tomer and Mansour, Yishay
author:
- given: Aadirupa
  family: Saha
- given: Tomer
  family: Koren
- given: Yishay
  family: Mansour
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/saha21b/saha21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/saha21b/saha21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
