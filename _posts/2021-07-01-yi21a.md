---
title: Improved OOD Generalization via Adversarial Training and Pretraing
abstract: Recently, learning a model that generalizes well on out-of-distribution
  (OOD) data has attracted great attention in the machine learning community. In this
  paper, after defining OOD generalization by Wasserstein distance, we theoretically
  justify that a model robust to input perturbation also generalizes well on OOD data.
  Inspired by previous findings that adversarial training helps improve robustness,
  we show that models trained by adversarial training have converged excess risk on
  OOD data. Besides, in the paradigm of pre-training then fine-tuning, we theoretically
  justify that the input perturbation robust model in the pre-training stage provides
  an initialization that generalizes well on downstream OOD data. Finally, various
  experiments conducted on image classification and natural language understanding
  tasks verify our theoretical findings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yi21a
month: 0
tex_title: Improved OOD Generalization via Adversarial Training and Pretraing
firstpage: 11987
lastpage: 11997
page: 11987-11997
order: 11987
cycles: false
bibtex_author: Yi, Mingyang and Hou, Lu and Sun, Jiacheng and Shang, Lifeng and Jiang,
  Xin and Liu, Qun and Ma, Zhiming
author:
- given: Mingyang
  family: Yi
- given: Lu
  family: Hou
- given: Jiacheng
  family: Sun
- given: Lifeng
  family: Shang
- given: Xin
  family: Jiang
- given: Qun
  family: Liu
- given: Zhiming
  family: Ma
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/yi21a/yi21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/yi21a/yi21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
