---
title: Improved Contrastive Divergence Training of Energy-Based Models
abstract: Contrastive divergence is a popular method of training energy-based models,
  but is known to have difficulties with training stability. We propose an adaptation
  to improve contrastive divergence training by scrutinizing a gradient term that
  is difficult to calculate and is often left out for convenience. We show that this
  gradient term is numerically significant and in practice is important to avoid training
  instabilities, while being tractable to estimate. We further highlight how data
  augmentation and multi-scale processing can be used to improve model robustness
  and generation quality. Finally, we empirically evaluate stability of model architectures
  and show improved performance on a host of benchmarks and use cases, such as image
  generation, OOD detection, and compositional generation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: du21b
month: 0
tex_title: Improved Contrastive Divergence Training of Energy-Based Models
firstpage: 2837
lastpage: 2848
page: 2837-2848
order: 2837
cycles: false
bibtex_author: Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor
author:
- given: Yilun
  family: Du
- given: Shuang
  family: Li
- given: Joshua
  family: Tenenbaum
- given: Igor
  family: Mordatch
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/du21b/du21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/du21b/du21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
