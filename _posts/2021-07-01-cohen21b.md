---
title: Scaling Properties of Deep Residual Networks
abstract: Residual networks (ResNets) have displayed impressive results in pattern
  recognition and, recently, have garnered considerable theoretical interest due to
  a perceived link with neural ordinary differential equations (neural ODEs). This
  link relies on the convergence of network weights to a smooth function as the number
  of layers increases. We investigate the properties of weights trained by stochastic
  gradient descent and their scaling with network depth through detailed numerical
  experiments. We observe the existence of scaling regimes markedly different from
  those assumed in neural ODE literature. Depending on certain features of the network
  architecture, such as the smoothness of the activation function, one may obtain
  an alternative ODE limit, a stochastic differential equation or neither of these.
  These findings cast doubts on the validity of the neural ODE model as an adequate
  asymptotic description of deep ResNets and point to an alternative class of differential
  equations as a better description of the deep network limit.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cohen21b
month: 0
tex_title: Scaling Properties of Deep Residual Networks
firstpage: 2039
lastpage: 2048
page: 2039-2048
order: 2039
cycles: false
bibtex_author: Cohen, Alain-Sam and Cont, Rama and Rossier, Alain and Xu, Renyuan
author:
- given: Alain-Sam
  family: Cohen
- given: Rama
  family: Cont
- given: Alain
  family: Rossier
- given: Renyuan
  family: Xu
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/cohen21b/cohen21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/cohen21b/cohen21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
