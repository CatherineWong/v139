---
title: 'Learning by Turning: Neural Architecture Aware Optimisation'
abstract: 'Descent methods for deep networks are notoriously capricious: they require
  careful tuning of step size, momentum and weight decay, and which method will work
  best on a new benchmark is a priori unclear. To address this problem, this paper
  conducts a combined study of neural architecture and optimisation, leading to a
  new optimiser called Nero: the neuronal rotator. Nero trains reliably without momentum
  or weight decay, works in situations where Adam and SGD fail, and requires little
  to no learning rate tuning. Also, Nero’s memory footprint is   square root that
  of Adam or LAMB. Nero combines two ideas: (1) projected gradient descent over the
  space of balanced networks; (2) neuron-specific updates, where the step size sets
  the angle through which each neuron’s hyperplane turns. The paper concludes by discussing
  how this geometric connection between architecture and optimisation may impact theories
  of generalisation in deep learning.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu21c
month: 0
tex_title: 'Learning by Turning: Neural Architecture Aware Optimisation'
firstpage: 6748
lastpage: 6758
page: 6748-6758
order: 6748
cycles: false
bibtex_author: Liu, Yang and Bernstein, Jeremy and Meister, Markus and Yue, Yisong
author:
- given: Yang
  family: Liu
- given: Jeremy
  family: Bernstein
- given: Markus
  family: Meister
- given: Yisong
  family: Yue
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/liu21c/liu21c.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/liu21c/liu21c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
