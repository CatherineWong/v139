---
title: 'PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration'
abstract: Model-based Reinforcement Learning (RL) is a popular learning paradigm due
  to its potential sample efficiency compared to model-free RL. However, existing
  empirical model-based RL approaches lack the ability to explore. This work studies
  a computationally and statistically efficient model-based algorithm for both Kernelized
  Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). For both
  models, our algorithm guarantees polynomial sample complexity and only uses access
  to a planning oracle. Experimentally, we first demonstrate the flexibility and the
  efficacy of our algorithm on a set of exploration challenging control tasks where
  existing empirical model-based RL approaches completely fail. We then show that
  our approach retains excellent performance even in common dense reward control benchmarks
  that do not require heavy exploration.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: song21b
month: 0
tex_title: 'PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration'
firstpage: 9801
lastpage: 9811
page: 9801-9811
order: 9801
cycles: false
bibtex_author: Song, Yuda and Sun, Wen
author:
- given: Yuda
  family: Song
- given: Wen
  family: Sun
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/song21b/song21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/song21b/song21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
