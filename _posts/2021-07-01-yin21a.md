---
title: Distributed Nyström Kernel Learning with Communications
abstract: We study the statistical performance for distributed kernel ridge regression
  with Nyström (DKRR-NY) and with Nyström and iterative solvers (DKRR-NY-PCG) and
  successfully derive the optimal learning rates, which can improve the ranges of
  the number of local processors $p$ to the optimal in existing state-of-art bounds.
  More precisely, our theoretical analysis show that DKRR-NY and DKRR-NY-PCG achieve
  the same learning rates as the exact KRR requiring essentially $\mathcal{O}(|D|^{1.5})$
  time and $\mathcal{O}(|D|)$ memory with relaxing the restriction on $p$ in expectation,
  where $|D|$ is the number of data, which exhibits the average effectiveness of multiple
  trials. Furthermore, for showing the generalization performance in a single trial,
  we deduce the learning rates for DKRR-NY and DKRR-NY-PCG in probability. Finally,
  we propose a novel algorithm DKRR-NY-CM based on DKRR-NY, which employs a communication
  strategy to further improve the learning performance, whose effectiveness of communications
  is validated in theoretical and experimental analysis.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yin21a
month: 0
tex_title: Distributed Nyström Kernel Learning with Communications
firstpage: 12019
lastpage: 12028
page: 12019-12028
order: 12019
cycles: false
bibtex_author: Yin, Rong and Wang, Weiping and Meng, Dan
author:
- given: Rong
  family: Yin
- given: Weiping
  family: Wang
- given: Dan
  family: Meng
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/yin21a/yin21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/yin21a/yin21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
