---
title: Policy Caches with Successor Features
abstract: Transfer in reinforcement learning is based on the idea that it is possible
  to use what is learned in one task to improve the learning process in another task.
  For transfer between tasks which share transition dynamics but differ in reward
  function, successor features have been shown to be a useful representation which
  allows for efficient computation of action-value functions for previously-learned
  policies in new tasks. These functions induce policies in the new tasks, so an agent
  may not need to learn a new policy for each new task it encounters, especially if
  it is allowed some amount of suboptimality in those tasks. We present new bounds
  for the performance of optimal policies in a new task, as well as an approach to
  use these bounds to decide, when presented with a new task, whether to use cached
  policies or learn a new policy.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nemecek21a
month: 0
tex_title: Policy Caches with Successor Features
firstpage: 8025
lastpage: 8033
page: 8025-8033
order: 8025
cycles: false
bibtex_author: Nemecek, Mark and Parr, Ronald
author:
- given: Mark
  family: Nemecek
- given: Ronald
  family: Parr
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/nemecek21a/nemecek21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/nemecek21a/nemecek21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
