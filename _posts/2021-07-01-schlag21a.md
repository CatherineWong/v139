---
title: Linear Transformers Are Secretly Fast Weight Programmers
abstract: We show the formal equivalence of linearised self-attention mechanisms and
  fast weight controllers from the early ’90s, where a slow neural net learns by gradient
  descent to program the fast weights of another net through sequences of elementary
  programming instructions which are additive outer products of self-invented activation
  patterns (today called keys and values). Such Fast Weight Programmers (FWPs) learn
  to manipulate the contents of a finite memory and dynamically interact with it.
  We infer a memory capacity limitation of recent linearised softmax attention variants,
  and replace the purely additive outer products by a delta rule-like programming
  instruction, such that the FWP can more easily learn to correct the current mapping
  from keys to values. The FWP also learns to compute dynamically changing learning
  rates. We also propose a new kernel function to linearise attention which balances
  simplicity and effectiveness. We conduct experiments on synthetic retrieval problems
  as well as standard machine translation and language modelling tasks which demonstrate
  the benefits of our methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: schlag21a
month: 0
tex_title: Linear Transformers Are Secretly Fast Weight Programmers
firstpage: 9355
lastpage: 9366
page: 9355-9366
order: 9355
cycles: false
bibtex_author: Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen
author:
- given: Imanol
  family: Schlag
- given: Kazuki
  family: Irie
- given: Jürgen
  family: Schmidhuber
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/schlag21a/schlag21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/schlag21a/schlag21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
