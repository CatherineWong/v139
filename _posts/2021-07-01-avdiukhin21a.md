---
title: Federated Learning under Arbitrary Communication Patterns
abstract: 'Federated Learning is a distributed learning setting where the goal is
  to train a centralized model with training data distributed over a large number
  of heterogeneous clients, each with unreliable and relatively slow network connections.
  A common optimization approach used in federated learning is based on the idea of
  local SGD: each client runs some number of SGD steps locally and then the updated
  local models are averaged to form the updated global model on the coordinating server.
  In this paper, we investigate the performance of an asynchronous version of local
  SGD wherein the clients can communicate with the server at arbitrary time intervals.
  Our main result shows that for smooth strongly convex and smooth nonconvex functions
  we achieve convergence rates that match the synchronous version that requires all
  clients to communicate simultaneously.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: avdiukhin21a
month: 0
tex_title: Federated Learning under Arbitrary Communication Patterns
firstpage: 425
lastpage: 435
page: 425-435
order: 425
cycles: false
bibtex_author: Avdiukhin, Dmitrii and Kasiviswanathan, Shiva
author:
- given: Dmitrii
  family: Avdiukhin
- given: Shiva
  family: Kasiviswanathan
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/avdiukhin21a/avdiukhin21a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v139/avdiukhin21a/avdiukhin21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
