---
title: Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio
  Estimation
abstract: Density ratio estimation (DRE) is at the core of various machine learning
  tasks such as anomaly detection and domain adaptation. In the DRE literature, existing
  studies have extensively studied methods based on Bregman divergence (BD) minimization.
  However, when we apply the BD minimization with highly flexible models, such as
  deep neural networks, it tends to suffer from what we call train-loss hacking, which
  is a source of over-fitting caused by a typical characteristic of empirical BD estimators.
  In this paper, to mitigate train-loss hacking, we propose non-negative correction
  for empirical BD estimators. Theoretically, we confirm the soundness of the proposed
  method through a generalization error bound. In our experiments, the proposed methods
  show favorable performances in inlier-based outlier detection.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kato21a
month: 0
tex_title: Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio
  Estimation
firstpage: 5320
lastpage: 5333
page: 5320-5333
order: 5320
cycles: false
bibtex_author: Kato, Masahiro and Teshima, Takeshi
author:
- given: Masahiro
  family: Kato
- given: Takeshi
  family: Teshima
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/kato21a/kato21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/kato21a/kato21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
