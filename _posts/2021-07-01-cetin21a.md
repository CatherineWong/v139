---
title: Learning Routines for Effective Off-Policy Reinforcement Learning
abstract: 'The performance of reinforcement learning depends upon designing an appropriate
  action space, where the effect of each action is measurable, yet, granular enough
  to permit flexible behavior. So far, this process involved non-trivial user choices
  in terms of the available actions and their execution frequency. We propose a novel
  framework for reinforcement learning that effectively lifts such constraints. Within
  our framework, agents learn effective behavior over a routine space: a new, higher-level
  action space, where each routine represents a set of ’equivalent’ sequences of granular
  actions with arbitrary length. Our routine space is learned end-to-end to facilitate
  the accomplishment of underlying off-policy reinforcement learning objectives. We
  apply our framework to two state-of-the-art off-policy algorithms and show that
  the resulting agents obtain relevant performance improvements while requiring fewer
  interactions with the environment per episode, improving computational efficiency.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cetin21a
month: 0
tex_title: Learning Routines for Effective Off-Policy Reinforcement Learning
firstpage: 1384
lastpage: 1394
page: 1384-1394
order: 1384
cycles: false
bibtex_author: Cetin, Edoardo and Celiktutan, Oya
author:
- given: Edoardo
  family: Cetin
- given: Oya
  family: Celiktutan
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/cetin21a/cetin21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/cetin21a/cetin21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
