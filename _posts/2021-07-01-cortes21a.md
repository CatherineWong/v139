---
title: Relative Deviation Margin Bounds
abstract: We present a series of new and more favorable margin-based learning guarantees
  that depend on the empirical margin loss of a predictor. e give two types of learning
  bounds, in terms of either the Rademacher complexity or the empirical $\ell_\infty$-covering
  number of the hypothesis set used, both distribution-dependent and valid for general
  families. Furthermore, using our relative deviation margin bounds, we derive distribution-dependent
  generalization bounds for unbounded loss functions under the assumption of a finite
  moment. We also briefly highlight several applications of these bounds and discuss
  their connection with existing results.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cortes21a
month: 0
tex_title: Relative Deviation Margin Bounds
firstpage: 2122
lastpage: 2131
page: 2122-2131
order: 2122
cycles: false
bibtex_author: Cortes, Corinna and Mohri, Mehryar and Suresh, Ananda Theertha
author:
- given: Corinna
  family: Cortes
- given: Mehryar
  family: Mohri
- given: Ananda Theertha
  family: Suresh
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/cortes21a/cortes21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/cortes21a/cortes21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
