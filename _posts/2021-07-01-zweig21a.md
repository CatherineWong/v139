---
title: A Functional Perspective on Learning Symmetric Functions with Neural Networks
abstract: Symmetric functions, which take as input an unordered, fixed-size set, are
  known to be universally representable by neural networks that enforce permutation
  invariance. These architectures only give guarantees for fixed input sizes, yet
  in many practical applications, including point clouds and particle physics, a relevant
  notion of generalization should include varying the input size. In this work we
  treat symmetric functions (of any size) as functions over probability measures,
  and study the learning and representation of neural networks defined on measures.
  By focusing on shallow architectures, we establish approximation and generalization
  bounds under different choices of regularization (such as RKHS and variation norms),
  that capture a hierarchy of functional spaces with increasing degree of non-linear
  learning. The resulting models can be learned efficiently and enjoy generalization
  guarantees that extend across input sizes, as we verify empirically.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zweig21a
month: 0
tex_title: A Functional Perspective on Learning Symmetric Functions with Neural Networks
firstpage: 13023
lastpage: 13032
page: 13023-13032
order: 13023
cycles: false
bibtex_author: Zweig, Aaron and Bruna, Joan
author:
- given: Aaron
  family: Zweig
- given: Joan
  family: Bruna
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zweig21a/zweig21a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v139/zweig21a/zweig21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
