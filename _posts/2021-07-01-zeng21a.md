---
title: 'You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling'
abstract: Transformer-based models are widely used in natural language processing
  (NLP). Central to the transformer model is the self-attention mechanism, which captures
  the interactions of token pairs in the input sequences and depends quadratically
  on the sequence length. Training such models on longer sequences is expensive. In
  this paper, we show that a Bernoulli sampling attention mechanism based on Locality
  Sensitive Hashing (LSH), decreases the quadratic complexity of such models to linear.
  We bypass the quadratic cost by considering self-attention as a sum of individual
  tokens associated with Bernoulli random variables that can, in principle, be sampled
  at once by a single hash (although in practice, this number may be a small constant).
  This leads to an efficient sampling scheme to estimate self-attention which relies
  on specific modifications of LSH (to enable deployment on GPU architectures). We
  evaluate our algorithm on the GLUE benchmark with standard 512 sequence length where
  we see favorable performance relative to a standard pretrained Transformer. On the
  Long Range Arena (LRA) benchmark, for evaluating performance on long sequences,
  our method achieves results consistent with softmax self-attention but with sizable
  speed-ups and memory savings and often outperforms other efficient self-attention
  methods. Our code is available at https://github.com/mlpen/YOSO.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zeng21a
month: 0
tex_title: 'You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli
  Sampling'
firstpage: 12321
lastpage: 12332
page: 12321-12332
order: 12321
cycles: false
bibtex_author: Zeng, Zhanpeng and Xiong, Yunyang and Ravi, Sathya and Acharya, Shailesh
  and Fung, Glenn M and Singh, Vikas
author:
- given: Zhanpeng
  family: Zeng
- given: Yunyang
  family: Xiong
- given: Sathya
  family: Ravi
- given: Shailesh
  family: Acharya
- given: Glenn M
  family: Fung
- given: Vikas
  family: Singh
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zeng21a/zeng21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/zeng21a/zeng21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
