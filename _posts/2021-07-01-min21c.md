---
title: On the Explicit Role of Initialization on the Convergence and Implicit Bias
  of Overparametrized Linear Networks
abstract: Neural networks trained via gradient descent with random initialization
  and without any regularization enjoy good generalization performance in practice
  despite being highly overparametrized. A promising direction to explain this phenomenon
  is to study how initialization and overparametrization affect convergence and implicit
  bias of training algorithms. In this paper, we present a novel analysis of single-hidden-layer
  linear networks trained under gradient flow, which connects initialization, optimization,
  and overparametrization. Firstly, we show that the squared loss converges exponentially
  to its optimum at a rate that depends on the level of imbalance of the initialization.
  Secondly, we show that proper initialization constrains the dynamics of the network
  parameters to lie within an invariant set. In turn, minimizing the loss over this
  set leads to the min-norm solution. Finally, we show that large hidden layer width,
  together with (properly scaled) random initialization, ensures proximity to such
  an invariant set during training, allowing us to derive a novel non-asymptotic upper-bound
  on the distance between the trained network and the min-norm solution.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: min21c
month: 0
tex_title: On the Explicit Role of Initialization on the Convergence and Implicit
  Bias of Overparametrized Linear Networks
firstpage: 7760
lastpage: 7768
page: 7760-7768
order: 7760
cycles: false
bibtex_author: Min, Hancheng and Tarmoun, Salma and Vidal, Rene and Mallada, Enrique
author:
- given: Hancheng
  family: Min
- given: Salma
  family: Tarmoun
- given: Rene
  family: Vidal
- given: Enrique
  family: Mallada
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/min21c/min21c.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/min21c/min21c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
