---
title: 'Posterior Value Functions: Hindsight Baselines for Policy Gradient Methods'
abstract: Hindsight allows reinforcement learning agents to leverage new observations
  to make inferences about earlier states and transitions. In this paper, we exploit
  the idea of hindsight and introduce posterior value functions. Posterior value functions
  are computed by inferring the posterior distribution over hidden components of the
  state in previous timesteps and can be used to construct novel unbiased baselines
  for policy gradient methods. Importantly, we prove that these baselines reduce (and
  never increase) the variance of policy gradient estimators compared to traditional
  state value functions. While the posterior value function is motivated by partial
  observability, we extend these results to arbitrary stochastic MDPs by showing that
  hindsight-capable agents can model stochasticity in the environment as a special
  case of partial observability. Finally, we introduce a pair of methods for learning
  posterior value functions and prove their convergence.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nota21a
month: 0
tex_title: 'Posterior Value Functions: Hindsight Baselines for Policy Gradient Methods'
firstpage: 8238
lastpage: 8247
page: 8238-8247
order: 8238
cycles: false
bibtex_author: Nota, Chris and Thomas, Philip and Silva, Bruno C. Da
author:
- given: Chris
  family: Nota
- given: Philip
  family: Thomas
- given: Bruno C. Da
  family: Silva
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/nota21a/nota21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/nota21a/nota21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
