---
title: On the Inherent Regularization Effects of Noise Injection During Training
abstract: Randomly perturbing networks during the training process is a commonly used
  approach to improving generalization performance. In this paper, we present a theoretical
  study of one particular way of random perturbation, which corresponds to injecting
  artificial noise to the training data. We provide a precise asymptotic characterization
  of the training and generalization errors of such randomly perturbed learning problems
  on a random feature model. Our analysis shows that Gaussian noise injection in the
  training process is equivalent to introducing a weighted ridge regularization, when
  the number of noise injections tends to infinity. The explicit form of the regularization
  is also given. Numerical results corroborate our asymptotic predictions, showing
  that they are accurate even in moderate problem dimensions. Our theoretical predictions
  are based on a new correlated Gaussian equivalence conjecture that generalizes recent
  results in the study of random feature models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dhifallah21a
month: 0
tex_title: On the Inherent Regularization Effects of Noise Injection During Training
firstpage: 2665
lastpage: 2675
page: 2665-2675
order: 2665
cycles: false
bibtex_author: Dhifallah, Oussama and Lu, Yue
author:
- given: Oussama
  family: Dhifallah
- given: Yue
  family: Lu
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/dhifallah21a/dhifallah21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/dhifallah21a/dhifallah21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
