---
title: 'FOP: Factorizing Optimal Joint Policy of Maximum-Entropy Multi-Agent Reinforcement
  Learning'
abstract: Value decomposition recently injects vigorous vitality into multi-agent
  actor-critic methods. However, existing decomposed actor-critic methods cannot guarantee
  the convergence of global optimum. In this paper, we present a novel multi-agent
  actor-critic method, FOP, which can factorize the optimal joint policy induced by
  maximum-entropy multi-agent reinforcement learning (MARL) into individual policies.
  Theoretically, we prove that factorized individual policies of FOP converge to the
  global optimum. Empirically, in the well-known matrix game and differential game,
  we verify that FOP can converge to the global optimum for both discrete and continuous
  action spaces. We also evaluate FOP on a set of StarCraft II micromanagement tasks,
  and demonstrate that FOP substantially outperforms state-of-the-art decomposed value-based
  and actor-critic methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21m
month: 0
tex_title: 'FOP: Factorizing Optimal Joint Policy of Maximum-Entropy Multi-Agent Reinforcement
  Learning'
firstpage: 12491
lastpage: 12500
page: 12491-12500
order: 12491
cycles: false
bibtex_author: Zhang, Tianhao and Li, Yueheng and Wang, Chen and Xie, Guangming and
  Lu, Zongqing
author:
- given: Tianhao
  family: Zhang
- given: Yueheng
  family: Li
- given: Chen
  family: Wang
- given: Guangming
  family: Xie
- given: Zongqing
  family: Lu
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zhang21m/zhang21m.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/zhang21m/zhang21m-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
