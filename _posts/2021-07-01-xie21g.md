---
title: 'Learning While Playing in Mean-Field Games: Convergence and Optimality'
abstract: We study reinforcement learning in mean-field games. To achieve the Nash
  equilibrium, which consists of a policy and a mean-field state, existing algorithms
  require obtaining the optimal policy while fixing any mean-field state. In practice,
  however, the policy and the mean-field state evolve simultaneously, as each agent
  is learning while playing. To bridge such a gap, we propose a fictitious play algorithm,
  which alternatively updates the policy (learning) and the mean-field state (playing)
  by one step of policy optimization and gradient descent, respectively. Despite the
  nonstationarity induced by such an alternating scheme, we prove that the proposed
  algorithm converges to the Nash equilibrium with an explicit convergence rate. To
  the best of our knowledge, it is the first provably efficient algorithm that achieves
  learning while playing via alternating updates.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xie21g
month: 0
tex_title: 'Learning While Playing in Mean-Field Games: Convergence and Optimality'
firstpage: 11436
lastpage: 11447
page: 11436-11447
order: 11436
cycles: false
bibtex_author: Xie, Qiaomin and Yang, Zhuoran and Wang, Zhaoran and Minca, Andreea
author:
- given: Qiaomin
  family: Xie
- given: Zhuoran
  family: Yang
- given: Zhaoran
  family: Wang
- given: Andreea
  family: Minca
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/xie21g/xie21g.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/xie21g/xie21g-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
