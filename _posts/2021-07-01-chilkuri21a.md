---
title: Parallelizing Legendre Memory Unit Training
abstract: Recently, a new recurrent neural network (RNN) named the Legendre Memory
  Unit (LMU) was proposed and shown to achieve state-of-the-art performance on several
  benchmark datasets. Here we leverage the linear time-invariant (LTI) memory component
  of the LMU to construct a simplified variant that can be parallelized during training
  (and yet executed as an RNN during inference), resulting in up to 200 times faster
  training. We note that our efficient parallelizing scheme is general and is applicable
  to any deep network whose recurrent components are linear dynamical systems. We
  demonstrate the improved accuracy of our new architecture compared to the original
  LMU and a variety of published LSTM and transformer networks across seven benchmarks.
  For instance, our LMU sets a new state-of-the-art result on psMNIST, and uses half
  the parameters while outperforming DistilBERT and LSTM models on IMDB sentiment
  analysis.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chilkuri21a
month: 0
tex_title: Parallelizing Legendre Memory Unit Training
firstpage: 1898
lastpage: 1907
page: 1898-1907
order: 1898
cycles: false
bibtex_author: Chilkuri, Narsimha Reddy and Eliasmith, Chris
author:
- given: Narsimha Reddy
  family: Chilkuri
- given: Chris
  family: Eliasmith
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/chilkuri21a/chilkuri21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/chilkuri21a/chilkuri21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
