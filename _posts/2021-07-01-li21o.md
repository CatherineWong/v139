---
title: Training Graph Neural Networks with 1000 Layers
abstract: Deep graph neural networks (GNNs) have achieved excellent results on various
  tasks on increasingly large graph datasets with millions of nodes and edges. However,
  memory complexity has become a major obstacle when training deep GNNs for practical
  applications due to the immense number of nodes, edges, and intermediate activations.
  To improve the scalability of GNNs, prior works propose smart graph sampling or
  partitioning strategies to train GNNs with a smaller set of nodes or sub-graphs.
  In this work, we study reversible connections, group convolutions, weight tying,
  and equilibrium models to advance the memory and parameter efficiency of GNNs. We
  find that reversible connections in combination with deep network architectures
  enable the training of overparameterized GNNs that significantly outperform existing
  methods on multiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels
  each) and RevGNN-Wide (448 layers with 224 channels each) were both trained on a
  single commodity GPU and achieve an ROC-AUC of 87.74 $\pm$ 0.13 and 88.14 $\pm$
  0.15 on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep is
  the deepest GNN in the literature by one order of magnitude.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li21o
month: 0
tex_title: Training Graph Neural Networks with 1000 Layers
firstpage: 6437
lastpage: 6449
page: 6437-6449
order: 6437
cycles: false
bibtex_author: Li, Guohao and M{\"u}ller, Matthias and Ghanem, Bernard and Koltun,
  Vladlen
author:
- given: Guohao
  family: Li
- given: Matthias
  family: MÃ¼ller
- given: Bernard
  family: Ghanem
- given: Vladlen
  family: Koltun
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/li21o/li21o.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/li21o/li21o-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
