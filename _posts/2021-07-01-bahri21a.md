---
title: Locally Adaptive Label Smoothing Improves Predictive Churn
abstract: Training modern neural networks is an inherently noisy process that can
  lead to high \emph{prediction churn}– disagreements between re-trainings of the
  same model due to factors such as randomization in the parameter initialization
  and mini-batches– even when the trained models all attain similar accuracies. Such
  prediction churn can be very undesirable in practice. In this paper, we present
  several baselines for reducing churn and show that training on soft labels obtained
  by adaptively smoothing each example’s label based on the example’s neighboring
  labels often outperforms the baselines on churn while improving accuracy on a variety
  of benchmark classification tasks and model architectures.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bahri21a
month: 0
tex_title: Locally Adaptive Label Smoothing Improves Predictive Churn
firstpage: 532
lastpage: 542
page: 532-542
order: 532
cycles: false
bibtex_author: Bahri, Dara and Jiang, Heinrich
author:
- given: Dara
  family: Bahri
- given: Heinrich
  family: Jiang
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/bahri21a/bahri21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/bahri21a/bahri21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
