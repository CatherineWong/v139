---
title: High Confidence Generalization for Reinforcement Learning
abstract: We present several classes of reinforcement learning algorithms that safely
  generalize to Markov decision processes (MDPs) not seen during training. Specifically,
  we study the setting in which some set of MDPs is accessible for training. The goal
  is to generalize safely to MDPs that are sampled from the same distribution, but
  which may not be in the set accessible for training. For various definitions of
  safety, our algorithms give probabilistic guarantees that agents can safely generalize
  to MDPs that are sampled from the same distribution but are not necessarily in the
  training set. These algorithms are a type of Seldonian algorithm (Thomas et al.,
  2019), which is a class of machine learning algorithms that return models with probabilistic
  safety guarantees for user-specified definitions of safety.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kostas21a
month: 0
tex_title: High Confidence Generalization for Reinforcement Learning
firstpage: 5764
lastpage: 5773
page: 5764-5773
order: 5764
cycles: false
bibtex_author: Kostas, James and Chandak, Yash and Jordan, Scott M and Theocharous,
  Georgios and Thomas, Philip
author:
- given: James
  family: Kostas
- given: Yash
  family: Chandak
- given: Scott M
  family: Jordan
- given: Georgios
  family: Theocharous
- given: Philip
  family: Thomas
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/kostas21a/kostas21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/kostas21a/kostas21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
