---
title: 'Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks'
abstract: As its width tends to infinity, a deep neural networkâ€™s behavior under gradient
  descent can become simplified and predictable (e.g. given by the Neural Tangent
  Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization).
  However, we show that the standard and NTK parametrizations of a neural network
  do not admit infinite-width limits that can *learn* features, which is crucial for
  pretraining and transfer learning such as with BERT. We propose simple modifications
  to the standard parametrization to allow for feature learning in the limit. Using
  the *Tensor Programs* technique, we derive explicit formulas for such limits. On
  Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely
  crucially on feature learning, we compute these limits exactly. We find that they
  outperform both NTK baselines and finite-width networks, with the latter approaching
  the infinite-width feature learning performance as width increases.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang21c
month: 0
tex_title: 'Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks'
firstpage: 11727
lastpage: 11737
page: 11727-11737
order: 11727
cycles: false
bibtex_author: Yang, Greg and Hu, Edward J.
author:
- given: Greg
  family: Yang
- given: Edward J.
  family: Hu
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/yang21c/yang21c.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/yang21c/yang21c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
