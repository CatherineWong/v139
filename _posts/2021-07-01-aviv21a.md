---
title: 'Asynchronous Distributed Learning : Adapting to Gradient Delays without Prior
  Knowledge'
abstract: We consider stochastic convex optimization problems, where several machines
  act asynchronously in parallel while sharing a common memory. We propose a robust
  training method for the constrained setting and derive non asymptotic convergence
  guarantees that do not depend on prior knowledge of update delays, objective smoothness,
  and gradient variance. Conversely, existing methods for this setting crucially rely
  on this prior knowledge, which render them unsuitable for essentially all shared-resources
  computational environments, such as clouds and data centers. Concretely, existing
  approaches are unable to accommodate changes in the delays which result from dynamic
  allocation of the machines, while our method implicitly adapts to such changes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: aviv21a
month: 0
tex_title: 'Asynchronous Distributed Learning : Adapting to Gradient Delays without
  Prior Knowledge'
firstpage: 436
lastpage: 445
page: 436-445
order: 436
cycles: false
bibtex_author: Aviv, Rotem Zamir and Hakimi, Ido and Schuster, Assaf and Levy, Kfir
  Yehuda
author:
- given: Rotem Zamir
  family: Aviv
- given: Ido
  family: Hakimi
- given: Assaf
  family: Schuster
- given: Kfir Yehuda
  family: Levy
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/aviv21a/aviv21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/aviv21a/aviv21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
