---
title: 'Tensor Programs IIb: Architectural Universality Of Neural Tangent Kernel Training
  Dynamics'
abstract: 'Yang (2020) recently showed that the Neural Tangent Kernel (NTK) at initialization
  has an infinite-width limit for a large class of architectures including modern
  staples such as ResNet and Transformers. However, their analysis does not apply
  to training. Here, we show the same neural networks (in the so-called NTK parametrization)
  during training follow a kernel gradient descent dynamics in function space, where
  the kernel is the infinite-width NTK. This completes the proof of the architectural
  universality of NTK behavior. To achieve this result, we apply the Tensor Programs
  technique: Write the entire SGD dynamics inside a Tensor Program and analyze it
  via the Master Theorem. To facilitate this proof, we develop a graphical notation
  for Tensor Programs, which we believe is also an important contribution toward the
  pedagogy and exposition of the Tensor Programs technique.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang21f
month: 0
tex_title: 'Tensor Programs IIb: Architectural Universality Of Neural Tangent Kernel
  Training Dynamics'
firstpage: 11762
lastpage: 11772
page: 11762-11772
order: 11762
cycles: false
bibtex_author: Yang, Greg and Littwin, Etai
author:
- given: Greg
  family: Yang
- given: Etai
  family: Littwin
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/yang21f/yang21f.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/yang21f/yang21f-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
