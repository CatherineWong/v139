---
title: Towards Understanding Learning in Neural Networks with Linear Teachers
abstract: Can a neural network minimizing cross-entropy learn linearly separable data?
  Despite progress in the theory of deep learning, this question remains unsolved.
  Here we prove that SGD globally optimizes this learning problem for a two-layer
  network with Leaky ReLU activations. The learned network can in principle be very
  complex. However, empirical evidence suggests that it often turns out to be approximately
  linear. We provide theoretical support for this phenomenon by proving that if network
  weights converge to two weight clusters, this will imply an approximately linear
  decision boundary. Finally, we show a condition on the optimization that leads to
  weight clustering. We provide empirical results that validate our theoretical analysis.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sarussi21a
month: 0
tex_title: Towards Understanding Learning in Neural Networks with Linear Teachers
firstpage: 9313
lastpage: 9322
page: 9313-9322
order: 9313
cycles: false
bibtex_author: Sarussi, Roei and Brutzkus, Alon and Globerson, Amir
author:
- given: Roei
  family: Sarussi
- given: Alon
  family: Brutzkus
- given: Amir
  family: Globerson
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/sarussi21a/sarussi21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/sarussi21a/sarussi21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
