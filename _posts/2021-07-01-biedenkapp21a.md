---
title: 'TempoRL: Learning When to Act'
abstract: Reinforcement learning is a powerful approach to learn behaviour through
  interactions with an environment. However, behaviours are usually learned in a purely
  reactive fashion, where an appropriate action is selected based on an observation.
  In this form, it is challenging to learn when it is necessary to execute new decisions.
  This makes learning inefficient especially in environments that need various degrees
  of fine and coarse control. To address this, we propose a proactive setting in which
  the agent not only selects an action in a state but also for how long to commit
  to that action. Our TempoRL approach introduces skip connections between states
  and learns a skip-policy for repeating the same action along these skips. We demonstrate
  the effectiveness of TempoRL on a variety of traditional and deep RL environments,
  showing that our approach is capable of learning successful policies up to an order
  of magnitude faster than vanilla Q-learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: biedenkapp21a
month: 0
tex_title: 'TempoRL: Learning When to Act'
firstpage: 914
lastpage: 924
page: 914-924
order: 914
cycles: false
bibtex_author: Biedenkapp, Andr{\'e} and Rajan, Raghu and Hutter, Frank and Lindauer,
  Marius
author:
- given: Andr√©
  family: Biedenkapp
- given: Raghu
  family: Rajan
- given: Frank
  family: Hutter
- given: Marius
  family: Lindauer
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/biedenkapp21a/biedenkapp21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/biedenkapp21a/biedenkapp21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
