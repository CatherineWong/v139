---
title: A Sharp Analysis of Model-based Reinforcement Learning with Self-Play
abstract: Model-based algorithms—algorithms that explore the environment through building
  and utilizing an estimated model—are widely used in reinforcement learning practice
  and theoretically shown to achieve optimal sample efficiency for single-agent reinforcement
  learning in Markov Decision Processes (MDPs). However, for multi-agent reinforcement
  learning in Markov games, the current best known sample complexity for model-based
  algorithms is rather suboptimal and compares unfavorably against recent model-free
  approaches. In this paper, we present a sharp analysis of model-based self-play
  algorithms for multi-agent Markov games. We design an algorithm \emph{Optimistic
  Nash Value Iteration} (Nash-VI) for two-player zero-sum Markov games that is able
  to output an $\epsilon$-approximate Nash policy in $\tilde{\mathcal{O}}(H^3SAB/\epsilon^2)$
  episodes of game playing, where $S$ is the number of states, $A,B$ are the number
  of actions for the two players respectively, and $H$ is the horizon length. This
  significantly improves over the best known model-based guarantee of $\tilde{\mathcal{O}}(H^4S^2AB/\epsilon^2)$,
  and is the first that matches the information-theoretic lower bound $\Omega(H^3S(A+B)/\epsilon^2)$
  except for a $\min\{A,B\}$ factor. In addition, our guarantee compares favorably
  against the best known model-free algorithm if $\min\{A,B\}=o(H^3)$, and outputs
  a single Markov policy while existing sample-efficient model-free algorithms output
  a nested mixture of Markov policies that is in general non-Markov and rather inconvenient
  to store and execute. We further adapt our analysis to designing a provably efficient
  task-agnostic algorithm for zero-sum Markov games, and designing the first line
  of provably sample-efficient algorithms for multi-player general-sum Markov games.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu21z
month: 0
tex_title: A Sharp Analysis of Model-based Reinforcement Learning with Self-Play
firstpage: 7001
lastpage: 7010
page: 7001-7010
order: 7001
cycles: false
bibtex_author: Liu, Qinghua and Yu, Tiancheng and Bai, Yu and Jin, Chi
author:
- given: Qinghua
  family: Liu
- given: Tiancheng
  family: Yu
- given: Yu
  family: Bai
- given: Chi
  family: Jin
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/liu21z/liu21z.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/liu21z/liu21z-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
