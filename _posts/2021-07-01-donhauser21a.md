---
title: How rotational invariance of common kernels prevents generalization in high
  dimensions
abstract: Kernel ridge regression is well-known to achieve minimax optimal rates in
  low-dimensional settings. However, its behavior in high dimensions is much less
  understood. Recent work establishes consistency for high-dimensional kernel regression
  for a number of specific assumptions on the data distribution. In this paper, we
  show that in high dimensions, the rotational invariance property of commonly studied
  kernels (such as RBF, inner product kernels and fully-connected NTK of any depth)
  leads to inconsistent estimation unless the ground truth is a low-degree polynomial.
  Our lower bound on the generalization error holds for a wide range of distributions
  and kernels with different eigenvalue decays. This lower bound suggests that consistency
  results for kernel ridge regression in high dimensions generally require a more
  refined analysis that depends on the structure of the kernel beyond its eigenvalue
  decay.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: donhauser21a
month: 0
tex_title: How rotational invariance of common kernels prevents generalization in
  high dimensions
firstpage: 2804
lastpage: 2814
page: 2804-2814
order: 2804
cycles: false
bibtex_author: Donhauser, Konstantin and Wu, Mingqi and Yang, Fanny
author:
- given: Konstantin
  family: Donhauser
- given: Mingqi
  family: Wu
- given: Fanny
  family: Yang
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/donhauser21a/donhauser21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/donhauser21a/donhauser21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
