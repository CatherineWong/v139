---
title: '1-bit Adam: Communication Efficient Large-Scale Training with Adam’s Convergence
  Speed'
abstract: Scalable training of large models (like BERT and GPT-3) requires careful
  optimization rooted in model design, architecture, and system capabilities. From
  a system standpoint, communication has become a major bottleneck, especially on
  commodity systems with standard TCP interconnects that offer limited network bandwidth.
  Communication compression is an important technique to reduce training time on such
  systems. One of the most effective ways to compress communication is via error compensation
  compression, which offers robust convergence speed, even under 1-bit compression.
  However, state-of-the-art error compensation techniques only work with basic optimizers
  like SGD and momentum SGD, which are linearly dependent on the gradients. They do
  not work with non-linear gradient-based optimizers like Adam, which offer state-of-the-art
  convergence efficiency and accuracy for models like BERT. In this paper, we propose
  1-bit Adam that reduces the communication volume by up to 5x, offers much better
  scalability, and provides the same convergence speed as uncompressed Adam. Our key
  finding is that Adam’s variance becomes stable (after a warmup phase) and can be
  used as a fixed precondition for the rest of the training (compression phase). We
  performed experiments on up to 256 GPUs and show that 1-bit Adam enables up to 3.3x
  higher throughput for BERT-Large pre-training and up to 2.9x higher throughput for
  SQuAD fine-tuning. In addition, we provide theoretical analysis for 1-bit Adam.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tang21a
month: 0
tex_title: '1-bit Adam: Communication Efficient Large-Scale Training with Adam’s Convergence
  Speed'
firstpage: 10118
lastpage: 10129
page: 10118-10129
order: 10118
cycles: false
bibtex_author: Tang, Hanlin and Gan, Shaoduo and Awan, Ammar Ahmad and Rajbhandari,
  Samyam and Li, Conglong and Lian, Xiangru and Liu, Ji and Zhang, Ce and He, Yuxiong
author:
- given: Hanlin
  family: Tang
- given: Shaoduo
  family: Gan
- given: Ammar Ahmad
  family: Awan
- given: Samyam
  family: Rajbhandari
- given: Conglong
  family: Li
- given: Xiangru
  family: Lian
- given: Ji
  family: Liu
- given: Ce
  family: Zhang
- given: Yuxiong
  family: He
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/tang21a/tang21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/tang21a/tang21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
