---
title: 'EL-Attention: Memory Efficient Lossless Attention for Generation'
abstract: Transformer model with multi-head attention requires caching intermediate
  results for efficient inference in generation tasks. However, cache brings new memory-related
  costs and prevents leveraging larger batch size for faster speed. We propose memory-efficient
  lossless attention (called EL-attention) to address this issue. It avoids heavy
  operations for building multi-head keys and values, cache for them is not needed.
  EL-attention constructs an ensemble of attention results by expanding query while
  keeping key and value shared. It produces the same result as multi-head attention
  with less GPU memory and faster inference speed. We conduct extensive experiments
  on Transformer, BART, and GPT-2 for summarization and question generation tasks.
  The results show EL-attention speeds up existing models by 1.6x to 5.3x without
  accuracy loss.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yan21a
month: 0
tex_title: 'EL-Attention: Memory Efficient Lossless Attention for Generation'
firstpage: 11648
lastpage: 11658
page: 11648-11658
order: 11648
cycles: false
bibtex_author: Yan, Yu and Chen, Jiusheng and Qi, Weizhen and Bhendawade, Nikhil and
  Gong, Yeyun and Duan, Nan and Zhang, Ruofei
author:
- given: Yu
  family: Yan
- given: Jiusheng
  family: Chen
- given: Weizhen
  family: Qi
- given: Nikhil
  family: Bhendawade
- given: Yeyun
  family: Gong
- given: Nan
  family: Duan
- given: Ruofei
  family: Zhang
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/yan21a/yan21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/yan21a/yan21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
