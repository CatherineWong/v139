---
title: Improved Denoising Diffusion Probabilistic Models
abstract: Denoising diffusion probabilistic models (DDPM) are a class of generative
  models which have recently been shown to produce excellent samples. We show that
  with a few simple modifications, DDPMs can also achieve competitive log-likelihoods
  while maintaining high sample quality. Additionally, we find that learning variances
  of the reverse diffusion process allows sampling with an order of magnitude fewer
  forward passes with a negligible difference in sample quality, which is important
  for the practical deployment of these models. We additionally use precision and
  recall to compare how well DDPMs and GANs cover the target distribution. Finally,
  we show that the sample quality and likelihood of these models scale smoothly with
  model capacity and training compute, making them easily scalable. We release our
  code and pre-trained models at https://github.com/openai/improved-diffusion.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nichol21a
month: 0
tex_title: Improved Denoising Diffusion Probabilistic Models
firstpage: 8162
lastpage: 8171
page: 8162-8171
order: 8162
cycles: false
bibtex_author: Nichol, Alexander Quinn and Dhariwal, Prafulla
author:
- given: Alexander Quinn
  family: Nichol
- given: Prafulla
  family: Dhariwal
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/nichol21a/nichol21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
