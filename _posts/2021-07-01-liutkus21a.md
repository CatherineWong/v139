---
title: Relative Positional Encoding for Transformers with Linear Complexity
abstract: Recent advances in Transformer models allow for unprecedented sequence lengths,
  due to linear space and time complexity. In the meantime, relative positional encoding
  (RPE) was proposed as beneficial for classical Transformers and consists in exploiting
  lags instead of absolute positions for inference. Still, RPE is not available for
  the recent linear-variants of the Transformer, because it requires the explicit
  computation of the attention matrix, which is precisely what is avoided by such
  methods. In this paper, we bridge this gap and present Stochastic Positional Encoding
  as a way to generate PE that can be used as a replacement to the classical additive
  (sinusoidal) PE and provably behaves like RPE. The main theoretical contribution
  is to make a connection between positional encoding and cross-covariance structures
  of correlated Gaussian processes. We illustrate the performance of our approach
  on the Long-Range Arena benchmark and on music generation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liutkus21a
month: 0
tex_title: Relative Positional Encoding for Transformers with Linear Complexity
firstpage: 7067
lastpage: 7079
page: 7067-7079
order: 7067
cycles: false
bibtex_author: Liutkus, Antoine and C\'{\i}fka, Ond{\v{r}}ej and Wu, Shih-Lun and
  Simsekli, Umut and Yang, Yi-Hsuan and Richard, Gael
author:
- given: Antoine
  family: Liutkus
- given: Ondřej
  family: Cı́fka
- given: Shih-Lun
  family: Wu
- given: Umut
  family: Simsekli
- given: Yi-Hsuan
  family: Yang
- given: Gael
  family: Richard
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/liutkus21a/liutkus21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/liutkus21a/liutkus21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
