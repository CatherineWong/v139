---
title: 'Oops I Took A Gradient: Scalable Sampling for Discrete Distributions'
abstract: We propose a general and scalable approximate sampling strategy for probabilistic
  models with discrete variables. Our approach uses gradients of the likelihood function
  with respect to its discrete inputs to propose updates in a Metropolis-Hastings
  sampler. We show empirically that this approach outperforms generic samplers in
  a number of difficult settings including Ising models, Potts models, restricted
  Boltzmann machines, and factorial hidden Markov models. We also demonstrate our
  improved sampler for training deep energy-based models on high dimensional discrete
  image data. This approach outperforms variational auto-encoders and existing energy-based
  models. Finally, we give bounds showing that our approach is near-optimal in the
  class of samplers which propose local updates.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: grathwohl21a
month: 0
tex_title: 'Oops I Took A Gradient: Scalable Sampling for Discrete Distributions'
firstpage: 3831
lastpage: 3841
page: 3831-3841
order: 3831
cycles: false
bibtex_author: Grathwohl, Will and Swersky, Kevin and Hashemi, Milad and Duvenaud,
  David and Maddison, Chris
author:
- given: Will
  family: Grathwohl
- given: Kevin
  family: Swersky
- given: Milad
  family: Hashemi
- given: David
  family: Duvenaud
- given: Chris
  family: Maddison
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/grathwohl21a/grathwohl21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/grathwohl21a/grathwohl21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
