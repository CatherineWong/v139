---
title: 'Synthesizer: Rethinking Self-Attention for Transformer Models'
abstract: The dot product self-attention is known to be central and indispensable
  to state-of-the-art Transformer models. But is it really required? This paper investigates
  the true importance and contribution of the dot product-based self-attention mechanism
  on the performance of Transformer models. Via extensive experiments, we find that
  (1) random alignment matrices surprisingly perform quite competitively and (2) learning
  attention weights from token-token (query-key) interactions is useful but not that
  important after all. To this end, we propose \textsc{Synthesizer}, a model that
  learns synthetic attention weights without token-token interactions. In our experiments,
  we first show that simple Synthesizers achieve highly competitive performance when
  compared against vanilla Transformer models across a range of tasks, including machine
  translation, language modeling, text generation and GLUE/SuperGLUE benchmarks. When
  composed with dot product attention, we find that Synthesizers consistently outperform
  Transformers. Moreover, we conduct additional comparisons of Synthesizers against
  Dynamic Convolutions, showing that simple Random Synthesizer is not only $60%$ faster
  but also improves perplexity by a relative $3.5%$. Finally, we show that simple
  factorized Synthesizers can outperform Linformers on encoding only tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tay21a
month: 0
tex_title: 'Synthesizer: Rethinking Self-Attention for Transformer Models'
firstpage: 10183
lastpage: 10192
page: 10183-10192
order: 10183
cycles: false
bibtex_author: Tay, Yi and Bahri, Dara and Metzler, Donald and Juan, Da-Cheng and
  Zhao, Zhe and Zheng, Che
author:
- given: Yi
  family: Tay
- given: Dara
  family: Bahri
- given: Donald
  family: Metzler
- given: Da-Cheng
  family: Juan
- given: Zhe
  family: Zhao
- given: Che
  family: Zheng
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/tay21a/tay21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
