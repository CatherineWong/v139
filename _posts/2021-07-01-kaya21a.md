---
title: When Does Data Augmentation Help With Membership Inference Attacks?
abstract: Deep learning models often raise privacy concerns as they leak information
  about their training data. This leakage enables membership inference attacks (MIA)
  that can identify whether a data point was in a model’s training set. Research shows
  that some ’data augmentation’ mechanisms may reduce the risk by combatting a key
  factor increasing the leakage, overfitting. While many mechanisms exist, their effectiveness
  against MIAs and privacy properties have not been studied systematically. Employing
  two recent MIAs, we explore the lower bound on the risk in the absence of formal
  upper bounds. First, we evaluate 7 mechanisms and differential privacy, on three
  image classification tasks. We find that applying augmentation to increase the model’s
  utility does not mitigate the risk and protection comes with a utility penalty.
  Further, we also investigate why popular label smoothing mechanism consistently
  amplifies the risk. Finally, we propose ’loss-rank-correlation’ (LRC) metric to
  assess how similar the effects of different mechanisms are. This, for example, reveals
  the similarity of applying high-intensity augmentation against MIAs to simply reducing
  the training time. Our findings emphasize the utility-privacy trade-off and provide
  practical guidelines on using augmentation to manage the trade-off.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kaya21a
month: 0
tex_title: When Does Data Augmentation Help With Membership Inference Attacks?
firstpage: 5345
lastpage: 5355
page: 5345-5355
order: 5345
cycles: false
bibtex_author: Kaya, Yigitcan and Dumitras, Tudor
author:
- given: Yigitcan
  family: Kaya
- given: Tudor
  family: Dumitras
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/kaya21a/kaya21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/kaya21a/kaya21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
