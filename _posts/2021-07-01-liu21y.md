---
title: Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization
  in Sparse Training
abstract: In this paper, we introduce a new perspective on training deep neural networks
  capable of state-of-the-art performance without the need for the expensive over-parameterization
  by proposing the concept of In-Time Over-Parameterization (ITOP) in sparse training.
  By starting from a random sparse network and continuously exploring sparse connectivities
  during training, we can perform an Over-Parameterization over the course of training,
  closing the gap in the expressibility between sparse training and dense training.
  We further use ITOP to understand the underlying mechanism of Dynamic Sparse Training
  (DST) and discover that the benefits of DST come from its ability to consider across
  time all possible parameters when searching for the optimal sparse connectivity.
  As long as sufficient parameters have been reliably explored, DST can outperform
  the dense neural network by a large margin. We present a series of experiments to
  support our conjecture and achieve the state-of-the-art sparse training performance
  with ResNet-50 on ImageNet. More impressively, ITOP achieves dominant performance
  over the overparameterization-based sparse methods at extreme sparsities. When trained
  with ResNet-34 on CIFAR-100, ITOP can match the performance of the dense model at
  an extreme sparsity 98%.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu21y
month: 0
tex_title: Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization
  in Sparse Training
firstpage: 6989
lastpage: 7000
page: 6989-7000
order: 6989
cycles: false
bibtex_author: Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy,
  Mykola
author:
- given: Shiwei
  family: Liu
- given: Lu
  family: Yin
- given: Decebal Constantin
  family: Mocanu
- given: Mykola
  family: Pechenizkiy
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/liu21y/liu21y.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/liu21y/liu21y-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
