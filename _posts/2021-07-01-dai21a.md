---
title: Sliced Iterative Normalizing Flows
abstract: We develop an iterative (greedy) deep learning (DL) algorithm which is able
  to transform an arbitrary probability distribution function (PDF) into the target
  PDF. The model is based on iterative Optimal Transport of a series of 1D slices,
  matching on each slice the marginal PDF to the target. The axes of the orthogonal
  slices are chosen to maximize the PDF difference using Wasserstein distance at each
  iteration, which enables the algorithm to scale well to high dimensions. As special
  cases of this algorithm, we introduce two sliced iterative Normalizing Flow (SINF)
  models, which map from the data to the latent space (GIS) and vice versa (SIG).
  We show that SIG is able to generate high quality samples of image datasets, which
  match the GAN benchmarks, while GIS obtains competitive results on density estimation
  tasks compared to the density trained NFs, and is more stable, faster, and achieves
  higher p(x) when trained on small training sets. SINF approach deviates significantly
  from the current DL paradigm, as it is greedy and does not use concepts such as
  mini-batching, stochastic gradient descent and gradient back-propagation through
  deep layers.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dai21a
month: 0
tex_title: Sliced Iterative Normalizing Flows
firstpage: 2352
lastpage: 2364
page: 2352-2364
order: 2352
cycles: false
bibtex_author: Dai, Biwei and Seljak, Uros
author:
- given: Biwei
  family: Dai
- given: Uros
  family: Seljak
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/dai21a/dai21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/dai21a/dai21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
