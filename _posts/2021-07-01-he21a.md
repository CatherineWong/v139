---
title: 'PipeTransformer: Automated Elastic Pipelining for Distributed Training of
  Large-scale Models'
abstract: The size of Transformer models is growing at an unprecedented rate. It has
  taken less than one year to reach trillion-level parameters since the release of
  GPT-3 (175B). Training such models requires both substantial engineering efforts
  and enormous computing resources, which are luxuries most research teams cannot
  afford. In this paper, we propose PipeTransformer, which leverages automated elastic
  pipelining for efficient distributed training of Transformer models. In PipeTransformer,
  we design an adaptive on the fly freeze algorithm that can identify and freeze some
  layers gradually during training, and an elastic pipelining system that can dynamically
  allocate resources to train the remaining active layers. More specifically, PipeTransformer
  automatically excludes frozen layers from the pipeline, packs active layers into
  fewer GPUs, and forks more replicas to increase data-parallel width. We evaluate
  PipeTransformer using Vision Transformer (ViT) on ImageNet and BERT on SQuAD and
  GLUE datasets. Our results show that compared to the state-of-the-art baseline,
  PipeTransformer attains up to 2.83-fold speedup without losing accuracy. We also
  provide various performance analyses for a more comprehensive understanding of our
  algorithmic and system-wise design. Finally, we have modularized our training system
  with flexible APIs and made the source code publicly available at https://DistML.ai.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: he21a
month: 0
tex_title: 'PipeTransformer: Automated Elastic Pipelining for Distributed Training
  of Large-scale Models'
firstpage: 4150
lastpage: 4159
page: 4150-4159
order: 4150
cycles: false
bibtex_author: He, Chaoyang and Li, Shen and Soltanolkotabi, Mahdi and Avestimehr,
  Salman
author:
- given: Chaoyang
  family: He
- given: Shen
  family: Li
- given: Mahdi
  family: Soltanolkotabi
- given: Salman
  family: Avestimehr
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/he21a/he21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/he21a/he21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
