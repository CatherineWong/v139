---
title: On the Predictability of Pruning Across Scales
abstract: We show that the error of iteratively magnitude-pruned networks empirically
  follows a scaling law with interpretable coefficients that depend on the architecture
  and task. We functionally approximate the error of the pruned networks, showing
  it is predictable in terms of an invariant tying width, depth, and pruning level,
  such that networks of vastly different pruned densities are interchangeable. We
  demonstrate the accuracy of this approximation over orders of magnitude in depth,
  width, dataset size, and density. We show that the functional form holds (generalizes)
  for large scale data (e.g., ImageNet) and architectures (e.g., ResNets). As neural
  networks become ever larger and costlier to train, our findings suggest a framework
  for reasoning conceptually and analytically about a standard method for unstructured
  pruning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rosenfeld21a
month: 0
tex_title: On the Predictability of Pruning Across Scales
firstpage: 9075
lastpage: 9083
page: 9075-9083
order: 9075
cycles: false
bibtex_author: Rosenfeld, Jonathan S and Frankle, Jonathan and Carbin, Michael and
  Shavit, Nir
author:
- given: Jonathan S
  family: Rosenfeld
- given: Jonathan
  family: Frankle
- given: Michael
  family: Carbin
- given: Nir
  family: Shavit
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/rosenfeld21a/rosenfeld21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/rosenfeld21a/rosenfeld21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
