---
title: Federated Composite Optimization
abstract: Federated Learning (FL) is a distributed learning paradigm that scales on-device
  learning collaboratively and privately. Standard FL algorithms such as FEDAVG are
  primarily geared towards smooth unconstrained settings. In this paper, we study
  the Federated Composite Optimization (FCO) problem, in which the loss function contains
  a non-smooth regularizer. Such problems arise naturally in FL applications that
  involve sparsity, low-rank, monotonicity, or more general constraints. We first
  show that straightforward extensions of primal algorithms such as FedAvg are not
  well-suited for FCO since they suffer from the "curse of primal averaging," resulting
  in poor convergence. As a solution, we propose a new primal-dual algorithm, Federated
  Dual Averaging (FedDualAvg), which by employing a novel server dual averaging procedure
  circumvents the curse of primal averaging. Our theoretical analysis and empirical
  experiments demonstrate that FedDualAvg outperforms the other baselines.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yuan21d
month: 0
tex_title: Federated Composite Optimization
firstpage: 12253
lastpage: 12266
page: 12253-12266
order: 12253
cycles: false
bibtex_author: Yuan, Honglin and Zaheer, Manzil and Reddi, Sashank
author:
- given: Honglin
  family: Yuan
- given: Manzil
  family: Zaheer
- given: Sashank
  family: Reddi
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/yuan21d/yuan21d.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/yuan21d/yuan21d-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
