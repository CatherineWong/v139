---
title: Decoupling Value and Policy for Generalization in Reinforcement Learning
abstract: 'Standard deep reinforcement learning algorithms use a shared representation
  for the policy and value function, especially when training directly from images.
  However, we argue that more information is needed to accurately estimate the value
  function than to learn the optimal policy. Consequently, the use of a shared representation
  for the policy and value function can lead to overfitting. To alleviate this problem,
  we propose two approaches which are combined to create IDAAC: Invariant Decoupled
  Advantage Actor-Critic. First, IDAAC decouples the optimization of the policy and
  value function, using separate networks to model them. Second, it introduces an
  auxiliary loss which encourages the representation to be invariant to task-irrelevant
  properties of the environment. IDAAC shows good generalization to unseen environments,
  achieving a new state-of-the-art on the Procgen benchmark and outperforming popular
  methods on DeepMind Control tasks with distractors. Our implementation is available
  at https://github.com/rraileanu/idaac.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: raileanu21a
month: 0
tex_title: Decoupling Value and Policy for Generalization in Reinforcement Learning
firstpage: 8787
lastpage: 8798
page: 8787-8798
order: 8787
cycles: false
bibtex_author: Raileanu, Roberta and Fergus, Rob
author:
- given: Roberta
  family: Raileanu
- given: Rob
  family: Fergus
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/raileanu21a/raileanu21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/raileanu21a/raileanu21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
