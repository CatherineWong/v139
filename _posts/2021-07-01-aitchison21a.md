---
title: Deep kernel processes
abstract: We define deep kernel processes in which positive definite Gram matrices
  are progressively transformed by nonlinear kernel functions and by sampling from
  (inverse) Wishart distributions. Remarkably, we find that deep Gaussian processes
  (DGPs), Bayesian neural networks (BNNs), infinite BNNs, and infinite BNNs with bottlenecks
  can all be written as deep kernel processes. For DGPs the equivalence arises because
  the Gram matrix formed by the inner product of features is Wishart distributed,
  and as we show, standard isotropic kernels can be written entirely in terms of this
  Gram matrix â€” we do not need knowledge of the underlying features. We define a tractable
  deep kernel process, the deep inverse Wishart process, and give a doubly-stochastic
  inducing-point variational inference scheme that operates on the Gram matrices,
  not on the features, as in DGPs. We show that the deep inverse Wishart process gives
  superior performance to DGPs and infinite BNNs on fully-connected baselines.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: aitchison21a
month: 0
tex_title: Deep kernel processes
firstpage: 130
lastpage: 140
page: 130-140
order: 130
cycles: false
bibtex_author: Aitchison, Laurence and Yang, Adam and Ober, Sebastian W
author:
- given: Laurence
  family: Aitchison
- given: Adam
  family: Yang
- given: Sebastian W
  family: Ober
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/aitchison21a/aitchison21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/aitchison21a/aitchison21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
