---
title: 'Poolingformer: Long Document Modeling with Pooling Attention'
abstract: 'In this paper, we introduce a two-level attention schema, Poolingformer,
  for long document modeling. Its first level uses a smaller sliding window pattern
  to aggregate information from neighbors. Its second level employs a larger window
  to increase receptive fields with pooling attention to reduce both computational
  cost and memory consumption. We first evaluate Poolingformer on two long sequence
  QA tasks: the monolingual NQ and the multilingual TyDi QA. Experimental results
  show that Poolingformer sits atop three official leaderboards measured by F1, outperforming
  previous state-of-the-art models by 1.9 points (79.8 vs. 77.9) on NQ long answer,
  1.9 points (79.5 vs. 77.6) on TyDi QA passage answer, and 1.6 points (67.6 vs. 66.0)
  on TyDi QA minimal answer. We further evaluate Poolingformer on a long sequence
  summarization task. Experimental results on the arXiv benchmark continue to demonstrate
  its superior performance.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21h
month: 0
tex_title: 'Poolingformer: Long Document Modeling with Pooling Attention'
firstpage: 12437
lastpage: 12446
page: 12437-12446
order: 12437
cycles: false
bibtex_author: Zhang, Hang and Gong, Yeyun and Shen, Yelong and Li, Weisheng and Lv,
  Jiancheng and Duan, Nan and Chen, Weizhu
author:
- given: Hang
  family: Zhang
- given: Yeyun
  family: Gong
- given: Yelong
  family: Shen
- given: Weisheng
  family: Li
- given: Jiancheng
  family: Lv
- given: Nan
  family: Duan
- given: Weizhu
  family: Chen
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zhang21h/zhang21h.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
