---
title: Contrastive Learning Inverts the Data Generating Process
abstract: Contrastive learning has recently seen tremendous success in self-supervised
  learning. So far, however, it is largely unclear why the learned representations
  generalize so effectively to a large variety of downstream tasks. We here prove
  that feedforward models trained with objectives belonging to the commonly used InfoNCE
  family learn to implicitly invert the underlying generative model of the observed
  data. While the proofs make certain statistical assumptions about the generative
  model, we observe empirically that our findings hold even if these assumptions are
  severely violated. Our theory highlights a fundamental connection between contrastive
  learning, generative modeling, and nonlinear independent component analysis, thereby
  furthering our understanding of the learned representations as well as providing
  a theoretical foundation to derive more effective contrastive losses.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zimmermann21a
month: 0
tex_title: Contrastive Learning Inverts the Data Generating Process
firstpage: 12979
lastpage: 12990
page: 12979-12990
order: 12979
cycles: false
bibtex_author: Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge,
  Matthias and Brendel, Wieland
author:
- given: Roland S.
  family: Zimmermann
- given: Yash
  family: Sharma
- given: Steffen
  family: Schneider
- given: Matthias
  family: Bethge
- given: Wieland
  family: Brendel
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zimmermann21a/zimmermann21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/zimmermann21a/zimmermann21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
