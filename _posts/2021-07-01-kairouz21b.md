---
title: Practical and Private (Deep) Learning Without Sampling or Shuffling
abstract: We consider training models with differential privacy (DP) using mini-batch
  gradients. The existing state-of-the-art, Differentially Private Stochastic Gradient
  Descent (DP-SGD), requires \emph{privacy amplification by sampling or shuffling}
  to obtain the best privacy/accuracy/computation trade-offs. Unfortunately, the precise
  requirements on exact sampling and shuffling can be hard to obtain in important
  practical scenarios, particularly federated learning (FL). We design and analyze
  a DP variant of Follow-The-Regularized-Leader (DP-FTRL) that compares favorably
  (both theoretically and empirically) to amplified DP-SGD, while allowing for much
  more flexible data access patterns. DP-FTRL does not use any form of privacy amplification.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kairouz21b
month: 0
tex_title: Practical and Private (Deep) Learning Without Sampling or Shuffling
firstpage: 5213
lastpage: 5225
page: 5213-5225
order: 5213
cycles: false
bibtex_author: Kairouz, Peter and Mcmahan, Brendan and Song, Shuang and Thakkar, Om
  and Thakurta, Abhradeep and Xu, Zheng
author:
- given: Peter
  family: Kairouz
- given: Brendan
  family: Mcmahan
- given: Shuang
  family: Song
- given: Om
  family: Thakkar
- given: Abhradeep
  family: Thakurta
- given: Zheng
  family: Xu
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/kairouz21b/kairouz21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/kairouz21b/kairouz21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
