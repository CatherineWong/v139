---
title: Quantitative Understanding of VAE as a Non-linearly Scaled Isometric Embedding
abstract: Variational autoencoder (VAE) estimates the posterior parameters (mean and
  variance) of latent variables corresponding to each input data. While it is used
  for many tasks, the transparency of the model is still an underlying issue. This
  paper provides a quantitative understanding of VAE property through the differential
  geometric and information-theoretic interpretations of VAE. According to the Rate-distortion
  theory, the optimal transform coding is achieved by using an orthonormal transform
  with PCA basis where the transform space is isometric to the input. Considering
  the analogy of transform coding to VAE, we clarify theoretically and experimentally
  that VAE can be mapped to an implicit isometric embedding with a scale factor derived
  from the posterior parameter. As a result, we can estimate the data probabilities
  in the input space from the prior, loss metrics, and corresponding posterior parameters,
  and further, the quantitative importance of each latent variable can be evaluated
  like the eigenvalue of PCA.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nakagawa21a
month: 0
tex_title: Quantitative Understanding of VAE as a Non-linearly Scaled Isometric Embedding
firstpage: 7916
lastpage: 7926
page: 7916-7926
order: 7916
cycles: false
bibtex_author: Nakagawa, Akira and Kato, Keizo and Suzuki, Taiji
author:
- given: Akira
  family: Nakagawa
- given: Keizo
  family: Kato
- given: Taiji
  family: Suzuki
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/nakagawa21a/nakagawa21a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v139/nakagawa21a/nakagawa21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
