---
title: Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation
abstract: We propose a new training objective named order-agnostic cross entropy (OaXE)
  for fully non-autoregressive translation (NAT) models. OaXE improves the standard
  cross-entropy loss to ameliorate the effect of word reordering, which is a common
  source of the critical multimodality problem in NAT. Concretely, OaXE removes the
  penalty for word order errors, and computes the cross entropy loss based on the
  best possible alignment between model predictions and target tokens. Since the log
  loss is very sensitive to invalid references, we leverage cross entropy initialization
  and loss truncation to ensure the model focuses on a good part of the search space.
  Extensive experiments on major WMT benchmarks demonstrate that OaXE substantially
  improves translation performance, setting new state of the art for fully NAT models.
  Further analyses show that OaXE indeed alleviates the multimodality problem by reducing
  token repetitions and increasing prediction confidence. Our code, data, and trained
  models are available at https://github.com/tencent-ailab/ICML21_OAXE.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: du21c
month: 0
tex_title: Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation
firstpage: 2849
lastpage: 2859
page: 2849-2859
order: 2849
cycles: false
bibtex_author: Du, Cunxiao and Tu, Zhaopeng and Jiang, Jing
author:
- given: Cunxiao
  family: Du
- given: Zhaopeng
  family: Tu
- given: Jing
  family: Jiang
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/du21c/du21c.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/du21c/du21c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
