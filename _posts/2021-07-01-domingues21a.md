---
title: 'Kernel-Based Reinforcement Learning: A Finite-Time Analysis'
abstract: We consider the exploration-exploitation dilemma in finite-horizon reinforcement
  learning problems whose state-action space is endowed with a metric. We introduce
  Kernel-UCBVI, a model-based optimistic algorithm that leverages the smoothness of
  the MDP and a non-parametric kernel estimator of the rewards and transitions to
  efficiently balance exploration and exploitation. For problems with $K$ episodes
  and horizon $H$, we provide a regret bound of $\widetilde{O}\left( H^3 K^{\frac{2d}{2d+1}}\right)$,
  where $d$ is the covering dimension of the joint state-action space. This is the
  first regret bound for kernel-based RL using smoothing kernels, which requires very
  weak assumptions on the MDP and applies to a wide range of tasks. We empirically
  validate our approach in continuous MDPs with sparse rewards.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: domingues21a
month: 0
tex_title: 'Kernel-Based Reinforcement Learning: A Finite-Time Analysis'
firstpage: 2783
lastpage: 2792
page: 2783-2792
order: 2783
cycles: false
bibtex_author: Domingues, Omar Darwiche and Menard, Pierre and Pirotta, Matteo and
  Kaufmann, Emilie and Valko, Michal
author:
- given: Omar Darwiche
  family: Domingues
- given: Pierre
  family: Menard
- given: Matteo
  family: Pirotta
- given: Emilie
  family: Kaufmann
- given: Michal
  family: Valko
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/domingues21a/domingues21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/domingues21a/domingues21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
