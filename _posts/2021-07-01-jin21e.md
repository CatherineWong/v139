---
title: Is Pessimism Provably Efficient for Offline RL?
abstract: We study offline reinforcement learning (RL), which aims to learn an optimal
  policy based on a dataset collected a priori. Due to the lack of further interactions
  with the environment, offline RL suffers from the insufficient coverage of the dataset,
  which eludes most existing theoretical analysis. In this paper, we propose a pessimistic
  variant of the value iteration algorithm (PEVI), which incorporates an uncertainty
  quantifier as the penalty function. Such a penalty function simply flips the sign
  of the bonus function for promoting exploration in online RL, which makes it easily
  implementable and compatible with general function approximators. Without assuming
  the sufficient coverage of the dataset, we establish a data-dependent upper bound
  on the suboptimality of PEVI for general Markov decision processes (MDPs). When
  specialized to linear MDPs, it matches the information-theoretic lower bound up
  to multiplicative factors of the dimension and horizon. In other words, pessimism
  is not only provably efficient but also minimax optimal. In particular, given the
  dataset, the learned policy serves as the “best effort” among all policies, as no
  other policies can do better. Our theoretical analysis identifies the critical role
  of pessimism in eliminating a notion of spurious correlation, which emerges from
  the “irrelevant” trajectories that are less covered by the dataset and not informative
  for the optimal policy.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jin21e
month: 0
tex_title: Is Pessimism Provably Efficient for Offline RL?
firstpage: 5084
lastpage: 5096
page: 5084-5096
order: 5084
cycles: false
bibtex_author: Jin, Ying and Yang, Zhuoran and Wang, Zhaoran
author:
- given: Ying
  family: Jin
- given: Zhuoran
  family: Yang
- given: Zhaoran
  family: Wang
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/jin21e/jin21e.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/jin21e/jin21e-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
