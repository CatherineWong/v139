---
title: '"Hey, that’s not an ODE": Faster ODE Adjoints via Seminorms'
abstract: Neural differential equations may be trained by backpropagating gradients
  via the adjoint method, which is another differential equation typically solved
  using an adaptive-step-size numerical differential equation solver. A proposed step
  is accepted if its error, \emph{relative to some norm}, is sufficiently small; else
  it is rejected, the step is shrunk, and the process is repeated. Here, we demonstrate
  that the particular structure of the adjoint equations makes the usual choices of
  norm (such as $L^2$) unnecessarily stringent. By replacing it with a more appropriate
  (semi)norm, fewer steps are unnecessarily rejected and the backpropagation is made
  faster. This requires only minor code modifications. Experiments on a wide range
  of tasks—including time series, generative modeling, and physical control—demonstrate
  a median improvement of 40% fewer function evaluations. On some problems we see
  as much as 62% fewer function evaluations, so that the overall training time is
  roughly halved.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kidger21a
month: 0
tex_title: '"Hey, that’s not an ODE": Faster ODE Adjoints via Seminorms'
firstpage: 5443
lastpage: 5452
page: 5443-5452
order: 5443
cycles: false
bibtex_author: Kidger, Patrick and Chen, Ricky T. Q. and Lyons, Terry J
author:
- given: Patrick
  family: Kidger
- given: Ricky T. Q.
  family: Chen
- given: Terry J
  family: Lyons
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/kidger21a/kidger21a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v139/kidger21a/kidger21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
