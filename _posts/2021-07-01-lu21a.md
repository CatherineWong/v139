---
title: Optimal Complexity in Decentralized Training
abstract: Decentralization is a promising method of scaling up parallel machine learning
  systems. In this paper, we provide a tight lower bound on the iteration complexity
  for such methods in a stochastic non-convex setting. Our lower bound reveals a theoretical
  gap in known convergence rates of many existing decentralized training algorithms,
  such as D-PSGD. We prove by construction this lower bound is tight and achievable.
  Motivated by our insights, we further propose DeTAG, a practical gossip-style decentralized
  algorithm that achieves the lower bound with only a logarithm gap. Empirically,
  we compare DeTAG with other decentralized algorithms on image classification tasks,
  and we show DeTAG enjoys faster convergence compared to baselines, especially on
  unshuffled data and in sparse networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lu21a
month: 0
tex_title: Optimal Complexity in Decentralized Training
firstpage: 7111
lastpage: 7123
page: 7111-7123
order: 7111
cycles: false
bibtex_author: Lu, Yucheng and De Sa, Christopher
author:
- given: Yucheng
  family: Lu
- given: Christopher
  family: De Sa
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/lu21a/lu21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/lu21a/lu21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
