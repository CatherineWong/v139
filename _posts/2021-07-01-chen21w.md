---
title: Cyclically Equivariant Neural Decoders for Cyclic Codes
abstract: Neural decoders were introduced as a generalization of the classic Belief
  Propagation (BP) decoding algorithms, where the Trellis graph in the BP algorithm
  is viewed as a neural network, and the weights in the Trellis graph are optimized
  by training the neural network. In this work, we propose a novel neural decoder
  for cyclic codes by exploiting their cyclically invariant property. More precisely,
  we impose a shift invariant structure on the weights of our neural decoder so that
  any cyclic shift of inputs results in the same cyclic shift of outputs. Extensive
  simulations with BCH codes and punctured Reed-Muller (RM) codes show that our new
  decoder consistently outperforms previous neural decoders when decoding cyclic codes.
  Finally, we propose a list decoding procedure that can significantly reduce the
  decoding error probability for BCH codes and punctured RM codes. For certain high-rate
  codes, the gap between our list decoder and the Maximum Likelihood decoder is less
  than $0.1$dB. Code available at github.com/cyclicallyneuraldecoder
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen21w
month: 0
tex_title: Cyclically Equivariant Neural Decoders for Cyclic Codes
firstpage: 1771
lastpage: 1780
page: 1771-1780
order: 1771
cycles: false
bibtex_author: Chen, Xiangyu and Ye, Min
author:
- given: Xiangyu
  family: Chen
- given: Min
  family: Ye
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/chen21w/chen21w.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/chen21w/chen21w-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
