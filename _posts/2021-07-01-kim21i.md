---
title: The Lipschitz Constant of Self-Attention
abstract: Lipschitz constants of neural networks have been explored in various contexts
  in deep learning, such as provable adversarial robustness, estimating Wasserstein
  distance, stabilising training of GANs, and formulating invertible neural networks.
  Such works have focused on bounding the Lipschitz constant of fully connected or
  convolutional networks, composed of linear maps and pointwise non-linearities. In
  this paper, we investigate the Lipschitz constant of self-attention, a non-linear
  neural network module widely used in sequence modelling. We prove that the standard
  dot-product self-attention is not Lipschitz for unbounded input domain, and propose
  an alternative L2 self-attention that is Lipschitz. We derive an upper bound on
  the Lipschitz constant of L2 self-attention and provide empirical evidence for its
  asymptotic tightness. To demonstrate the practical relevance of our theoretical
  work, we formulate invertible self-attention and use it in a Transformer-based architecture
  for a character-level language modelling task.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim21i
month: 0
tex_title: The Lipschitz Constant of Self-Attention
firstpage: 5562
lastpage: 5571
page: 5562-5571
order: 5562
cycles: false
bibtex_author: Kim, Hyunjik and Papamakarios, George and Mnih, Andriy
author:
- given: Hyunjik
  family: Kim
- given: George
  family: Papamakarios
- given: Andriy
  family: Mnih
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/kim21i/kim21i.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/kim21i/kim21i-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
