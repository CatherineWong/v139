---
title: Continuous-time Model-based Reinforcement Learning
abstract: Model-based reinforcement learning (MBRL) approaches rely on discrete-time
  state transition models whereas physical systems and the vast majority of control
  tasks operate in continuous-time. To avoid time-discretization approximation of
  the underlying process, we propose a continuous-time MBRL framework based on a novel
  actor-critic method. Our approach also infers the unknown state evolution differentials
  with Bayesian neural ordinary differential equations (ODE) to account for epistemic
  uncertainty. We implement and test our method on a new ODE-RL suite that explicitly
  solves continuous-time control systems. Our experiments illustrate that the model
  is robust against irregular and noisy data, and can solve classic control problems
  in a sample-efficient manner.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yildiz21a
month: 0
tex_title: Continuous-time Model-based Reinforcement Learning
firstpage: 12009
lastpage: 12018
page: 12009-12018
order: 12009
cycles: false
bibtex_author: Yildiz, Cagatay and Heinonen, Markus and L{\"a}hdesm{\"a}ki, Harri
author:
- given: Cagatay
  family: Yildiz
- given: Markus
  family: Heinonen
- given: Harri
  family: Lähdesmäki
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/yildiz21a/yildiz21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/yildiz21a/yildiz21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
