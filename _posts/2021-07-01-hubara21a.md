---
title: Accurate Post Training Quantization With Small Calibration Sets
abstract: 'Lately, post-training quantization methods have gained considerable attention,
  as they are simple to use, and require only a small unlabeled calibration set. This
  small dataset cannot be used to fine-tune the model without significant over-fitting.
  Instead, these methods only use the calibration set to set the activations’ dynamic
  ranges. However, such methods always resulted in significant accuracy degradation,
  when used below 8-bits (except on small datasets). Here we aim to break the 8-bit
  barrier. To this end, we minimize the quantization errors of each layer or block
  separately by optimizing its parameters over the calibration set. We empirically
  demonstrate that this approach is: (1) much less susceptible to over-fitting than
  the standard fine-tuning approaches, and can be used even on a very small calibration
  set; and (2) more powerful than previous methods, which only set the activations’
  dynamic ranges. We suggest two flavors for our method, parallel and sequential aim
  for a fixed and flexible bit-width allocation. For the latter, we demonstrate how
  to optimally allocate the bit-widths for each layer, while constraining accuracy
  degradation or model compression by proposing a novel integer programming formulation.
  Finally, we suggest model global statistics tuning, to correct biases introduced
  during quantization. Together, these methods yield state-of-the-art results for
  both vision and text models. For instance, on ResNet50, we obtain less than 1% accuracy
  degradation — with 4-bit weights and activations in all layers, but first and last.
  The suggested methods are two orders of magnitude faster than the traditional Quantize
  Aware Training approach used for lower than 8-bit quantization. We open-sourced
  our code \textit{https://github.com/papers-submission/CalibTIP}.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hubara21a
month: 0
tex_title: Accurate Post Training Quantization With Small Calibration Sets
firstpage: 4466
lastpage: 4475
page: 4466-4475
order: 4466
cycles: false
bibtex_author: Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and
  Soudry, Daniel
author:
- given: Itay
  family: Hubara
- given: Yury
  family: Nahshan
- given: Yair
  family: Hanani
- given: Ron
  family: Banner
- given: Daniel
  family: Soudry
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/hubara21a/hubara21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/hubara21a/hubara21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
