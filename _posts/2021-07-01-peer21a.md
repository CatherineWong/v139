---
title: Ensemble Bootstrapping for Q-Learning
abstract: Q-learning (QL), a common reinforcement learning algorithm, suffers from
  over-estimation bias due to the maximization term in the optimal Bellman operator.
  This bias may lead to sub-optimal behavior. Double-Q-learning tackles this issue
  by utilizing two estimators, yet results in an under-estimation bias. Similar to
  over-estimation in Q-learning, in certain scenarios, the under-estimation bias may
  degrade performance. In this work, we introduce a new bias-reduced algorithm called
  Ensemble Bootstrapped Q-Learning (EBQL), a natural extension of Double-Q-learning
  to ensembles. We analyze our method both theoretically and empirically. Theoretically,
  we prove that EBQL-like updates yield lower MSE when estimating the maximal mean
  of a set of independent random variables. Empirically, we show that there exist
  domains where both over and under-estimation result in sub-optimal performance.
  Finally, We demonstrate the superior performance of a deep RL variant of EBQL over
  other deep QL algorithms for a suite of ATARI games.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: peer21a
month: 0
tex_title: Ensemble Bootstrapping for Q-Learning
firstpage: 8454
lastpage: 8463
page: 8454-8463
order: 8454
cycles: false
bibtex_author: Peer, Oren and Tessler, Chen and Merlis, Nadav and Meir, Ron
author:
- given: Oren
  family: Peer
- given: Chen
  family: Tessler
- given: Nadav
  family: Merlis
- given: Ron
  family: Meir
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/peer21a/peer21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/peer21a/peer21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
