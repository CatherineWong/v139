---
title: 'Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation'
abstract: Advanced large-scale neural language models have led to significant success
  in many language generation tasks. However, the most commonly used training objective,
  Maximum Likelihood Estimation (MLE), has been shown problematic, where the trained
  model prefers using dull and repetitive phrases. In this work, we introduce ScaleGrad,
  a modification straight to the gradient of the loss function, to remedy the degeneration
  issue of the standard MLE objective. By directly maneuvering the gradient information,
  ScaleGrad makes the model learn to use novel tokens. Empirical results show the
  effectiveness of our method not only in open-ended generation, but also in directed
  generation tasks. With the simplicity in architecture, our method can serve as a
  general training objective that is applicable to most of the neural text generation
  tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lin21b
month: 0
tex_title: 'Straight to the Gradient: Learning to Use Novel Tokens for Neural Text
  Generation'
firstpage: 6642
lastpage: 6653
page: 6642-6653
order: 6642
cycles: false
bibtex_author: Lin, Xiang and Han, Simeng and Joty, Shafiq
author:
- given: Xiang
  family: Lin
- given: Simeng
  family: Han
- given: Shafiq
  family: Joty
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/lin21b/lin21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/lin21b/lin21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
