---
title: A Lower Bound for the Sample Complexity of Inverse Reinforcement Learning
abstract: Inverse reinforcement learning (IRL) is the task of finding a reward function
  that generates a desired optimal policy for a given Markov Decision Process (MDP).
  This paper develops an information-theoretic lower bound for the sample complexity
  of the finite state, finite action IRL problem. A geometric construction of $\beta$-strict
  separable IRL problems using spherical codes is considered. Properties of the ensemble
  size as well as the Kullback-Leibler divergence between the generated trajectories
  are derived. The resulting ensemble is then used along with Fanoâ€™s inequality to
  derive a sample complexity lower bound of $O(n \log n)$, where $n$ is the number
  of states in the MDP.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: komanduru21a
month: 0
tex_title: A Lower Bound for the Sample Complexity of Inverse Reinforcement Learning
firstpage: 5676
lastpage: 5685
page: 5676-5685
order: 5676
cycles: false
bibtex_author: Komanduru, Abi and Honorio, Jean
author:
- given: Abi
  family: Komanduru
- given: Jean
  family: Honorio
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/komanduru21a/komanduru21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/komanduru21a/komanduru21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
