---
title: Elementary superexpressive activations
abstract: 'We call a finite family of activation functions \emph{superexpressive}
  if any multivariate continuous function can be approximated by a neural network
  that uses these activations and has a fixed architecture only depending on the number
  of input variables (i.e., to achieve any accuracy we only need to adjust the weights,
  without increasing the number of neurons). Previously, it was known that superexpressive
  activations exist, but their form was quite complex. We give examples of very simple
  superexpressive families: for example, we prove that the family $\{sin, arcsin\}$
  is superexpressive. We also show that most practical activations (not involving
  periodic functions) are not superexpressive.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yarotsky21a
month: 0
tex_title: Elementary superexpressive activations
firstpage: 11932
lastpage: 11940
page: 11932-11940
order: 11932
cycles: false
bibtex_author: Yarotsky, Dmitry
author:
- given: Dmitry
  family: Yarotsky
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/yarotsky21a/yarotsky21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
