---
title: Quantifying the Benefit of Using Differentiable Learning over Tangent Kernels
abstract: We study the relative power of learning with gradient descent on differentiable
  models, such as neural networks, versus using the corresponding tangent kernels.
  We show that under certain conditions, gradient descent achieves small error only
  if a related tangent kernel method achieves a non-trivial advantage over random
  guessing (a.k.a. weak learning), though this advantage might be very small even
  when gradient descent can achieve arbitrarily high accuracy. Complementing this,
  we show that without these conditions, gradient descent can in fact learn with small
  error even when no kernel method, in particular using the tangent kernel, can achieve
  a non-trivial advantage over random guessing.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: malach21a
month: 0
tex_title: Quantifying the Benefit of Using Differentiable Learning over Tangent Kernels
firstpage: 7379
lastpage: 7389
page: 7379-7389
order: 7379
cycles: false
bibtex_author: Malach, Eran and Kamath, Pritish and Abbe, Emmanuel and Srebro, Nathan
author:
- given: Eran
  family: Malach
- given: Pritish
  family: Kamath
- given: Emmanuel
  family: Abbe
- given: Nathan
  family: Srebro
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/malach21a/malach21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/malach21a/malach21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
