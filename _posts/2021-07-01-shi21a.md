---
title: 'SparseBERT: Rethinking the Importance Analysis in Self-attention'
abstract: Transformer-based models are popularly used in natural language processing
  (NLP). Its core component, self-attention, has aroused widespread interest. To understand
  the self-attention mechanism, a direct method is to visualize the attention map
  of a pre-trained model. Based on the patterns observed, a series of efficient Transformers
  with different sparse attention masks have been proposed. From a theoretical perspective,
  universal approximability of Transformer-based models is also recently proved. However,
  the above understanding and analysis of self-attention is based on a pre-trained
  model. To rethink the importance analysis in self-attention, we study the significance
  of different positions in attention matrix during pre-training. A surprising result
  is that diagonal elements in the attention map are the least important compared
  with other attention positions. We provide a proof showing that these diagonal elements
  can indeed be removed without deteriorating model performance. Furthermore, we propose
  a Differentiable Attention Mask (DAM) algorithm, which further guides the design
  of the SparseBERT. Extensive experiments verify our interesting findings and illustrate
  the effect of the proposed algorithm.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shi21a
month: 0
tex_title: 'SparseBERT: Rethinking the Importance Analysis in Self-attention'
firstpage: 9547
lastpage: 9557
page: 9547-9557
order: 9547
cycles: false
bibtex_author: Shi, Han and Gao, Jiahui and Ren, Xiaozhe and Xu, Hang and Liang, Xiaodan
  and Li, Zhenguo and Kwok, James Tin-Yau
author:
- given: Han
  family: Shi
- given: Jiahui
  family: Gao
- given: Xiaozhe
  family: Ren
- given: Hang
  family: Xu
- given: Xiaodan
  family: Liang
- given: Zhenguo
  family: Li
- given: James Tin-Yau
  family: Kwok
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/shi21a/shi21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/shi21a/shi21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
