---
title: 'EfficientNetV2: Smaller Models and Faster Training'
abstract: This paper introduces EfficientNetV2, a new family of convolutional networks
  that have faster training speed and better parameter efficiency than previous models.
  To develop these models, we use a combination of training-aware neural architecture
  search and scaling, to jointly optimize training speed and parameter efficiency.
  The models were searched from the search space enriched with new ops such as Fused-MBConv.
  Our experiments show that EfficientNetV2 models train much faster than state-of-the-art
  models while being up to 6.8x smaller. Our training can be further sped up by progressively
  increasing the image size during training, but it often causes a drop in accuracy.
  To compensate for this accuracy drop, we propose an improved method of progressive
  learning, which adaptively adjusts regularization (e.g. data augmentation) along
  with image size. With progressive learning, our EfficientNetV2 significantly outperforms
  previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the
  same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012,
  outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using
  the same computing resources.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tan21a
month: 0
tex_title: 'EfficientNetV2: Smaller Models and Faster Training'
firstpage: 10096
lastpage: 10106
page: 10096-10106
order: 10096
cycles: false
bibtex_author: Tan, Mingxing and Le, Quoc
author:
- given: Mingxing
  family: Tan
- given: Quoc
  family: Le
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/tan21a/tan21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
