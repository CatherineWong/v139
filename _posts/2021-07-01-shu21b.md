---
title: 'Zoo-Tuning: Adaptive Transfer from A Zoo of Models'
abstract: With the development of deep networks on various large-scale datasets, a
  large zoo of pretrained models are available. When transferring from a model zoo,
  applying classic single-model-based transfer learning methods to each source model
  suffers from high computational cost and cannot fully utilize the rich knowledge
  in the zoo. We propose \emph{Zoo-Tuning} to address these challenges, which learns
  to adaptively transfer the parameters of pretrained models to the target task. With
  the learnable channel alignment layer and adaptive aggregation layer, Zoo-Tuning
  \emph{adaptively aggregates channel aligned pretrained parameters to derive the
  target model}, which simultaneously promotes knowledge transfer and adapts source
  models to downstream tasks. The adaptive aggregation substantially reduces the computation
  cost at both training and inference. We further propose lite Zoo-Tuning with the
  temporal ensemble of batch average gating values to reduce the storage cost at the
  inference time. We evaluate our approach on a variety of tasks, including reinforcement
  learning, image classification, and facial landmark detection. Experiment results
  demonstrate that the proposed adaptive transfer learning approach can more effectively
  and efficiently transfer knowledge from a zoo of models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shu21b
month: 0
tex_title: 'Zoo-Tuning: Adaptive Transfer from A Zoo of Models'
firstpage: 9626
lastpage: 9637
page: 9626-9637
order: 9626
cycles: false
bibtex_author: Shu, Yang and Kou, Zhi and Cao, Zhangjie and Wang, Jianmin and Long,
  Mingsheng
author:
- given: Yang
  family: Shu
- given: Zhi
  family: Kou
- given: Zhangjie
  family: Cao
- given: Jianmin
  family: Wang
- given: Mingsheng
  family: Long
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/shu21b/shu21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/shu21b/shu21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
