---
title: 'BASE Layers: Simplifying Training of Large, Sparse Models'
abstract: We introduce a new balanced assignment of experts (BASE) layer for large
  language models that greatly simplifies existing high capacity sparse layers. Sparse
  layers can dramatically improve the efficiency of training and inference by routing
  each token to specialized expert modules that contain only a small fraction of the
  model parameters. However, it can be difficult to learn balanced routing functions
  that make full use of the available experts; existing approaches typically use routing
  heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate
  token-to-expert allocation as a linear assignment problem, allowing an optimal assignment
  in which each expert receives an equal number of tokens. This optimal assignment
  scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies
  training by not requiring any new hyperparameters or auxiliary losses. Code is publicly
  released.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lewis21a
month: 0
tex_title: 'BASE Layers: Simplifying Training of Large, Sparse Models'
firstpage: 6265
lastpage: 6274
page: 6265-6274
order: 6265
cycles: false
bibtex_author: Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman
  and Zettlemoyer, Luke
author:
- given: Mike
  family: Lewis
- given: Shruti
  family: Bhosale
- given: Tim
  family: Dettmers
- given: Naman
  family: Goyal
- given: Luke
  family: Zettlemoyer
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/lewis21a/lewis21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
