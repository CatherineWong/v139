---
title: Evolving Attention with Residual Convolutions
abstract: Transformer is a ubiquitous model for natural language processing and has
  attracted wide attentions in computer vision. The attention maps are indispensable
  for a transformer model to encode the dependencies among input tokens. However,
  they are learned independently in each layer and sometimes fail to capture precise
  patterns. In this paper, we propose a novel and generic mechanism based on evolving
  attention to improve the performance of transformers. On one hand, the attention
  maps in different layers share common knowledge, thus the ones in preceding layers
  can instruct the attention in succeeding layers through residual connections. On
  the other hand, low-level and high-level attentions vary in the level of abstraction,
  so we adopt convolutional layers to model the evolutionary process of attention
  maps. The proposed evolving attention mechanism achieves significant performance
  improvement over various state-of-the-art models for multiple tasks, including image
  classification, natural language understanding and machine translation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang21ab
month: 0
tex_title: Evolving Attention with Residual Convolutions
firstpage: 10971
lastpage: 10980
page: 10971-10980
order: 10971
cycles: false
bibtex_author: Wang, Yujing and Yang, Yaming and Bai, Jiangang and Zhang, Mingliang
  and Bai, Jing and Yu, Jing and Zhang, Ce and Huang, Gao and Tong, Yunhai
author:
- given: Yujing
  family: Wang
- given: Yaming
  family: Yang
- given: Jiangang
  family: Bai
- given: Mingliang
  family: Zhang
- given: Jing
  family: Bai
- given: Jing
  family: Yu
- given: Ce
  family: Zhang
- given: Gao
  family: Huang
- given: Yunhai
  family: Tong
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/wang21ab/wang21ab.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/wang21ab/wang21ab-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
