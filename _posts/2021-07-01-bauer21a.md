---
title: Generalized Doubly Reparameterized Gradient Estimators
abstract: Efficient low-variance gradient estimation enabled by the reparameterization
  trick (RT) has been essential to the success of variational autoencoders. Doubly-reparameterized
  gradients (DReGs) improve on the RT for multi-sample variational bounds by applying
  reparameterization a second time for an additional reduction in variance. Here,
  we develop two generalizations of the DReGs estimator and show that they can be
  used to train conditional and hierarchical VAEs on image modelling tasks more effectively.
  We first extend the estimator to hierarchical models with several stochastic layers
  by showing how to treat additional score function terms due to the hierarchical
  variational posterior. We then generalize DReGs to score functions of arbitrary
  distributions instead of just those of the sampling distribution, which makes the
  estimator applicable to the parameters of the prior in addition to those of the
  posterior.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bauer21a
month: 0
tex_title: Generalized Doubly Reparameterized Gradient Estimators
firstpage: 738
lastpage: 747
page: 738-747
order: 738
cycles: false
bibtex_author: Bauer, Matthias and Mnih, Andriy
author:
- given: Matthias
  family: Bauer
- given: Andriy
  family: Mnih
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/bauer21a/bauer21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/bauer21a/bauer21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
