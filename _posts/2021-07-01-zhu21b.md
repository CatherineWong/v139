---
title: Data-Free Knowledge Distillation for Heterogeneous Federated Learning
abstract: Federated Learning (FL) is a decentralized machine-learning paradigm, in
  which a global server iteratively averages the model parameters of local users without
  accessing their data. User heterogeneity has imposed significant challenges to FL,
  which can incur drifted global models that are slow to converge. Knowledge Distillation
  has recently emerged to tackle this issue, by refining the server model using aggregated
  knowledge from heterogeneous users, other than directly averaging their model parameters.
  This approach, however, depends on a proxy dataset, making it impractical unless
  such a prerequisite is satisfied. Moreover, the ensemble knowledge is not fully
  utilized to guide local model learning, which may in turn affect the quality of
  the aggregated model. Inspired by the prior art, we propose a data-free knowledge
  distillation approach to address heterogeneous FL, where the server learns a lightweight
  generator to ensemble user information in a data-free manner, which is then broadcasted
  to users, regulating local training using the learned knowledge as an inductive
  bias. Empirical studies powered by theoretical implications show that our approach
  facilitates FL with better generalization performance using fewer communication
  rounds, compared with the state-of-the-art.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu21b
month: 0
tex_title: Data-Free Knowledge Distillation for Heterogeneous Federated Learning
firstpage: 12878
lastpage: 12889
page: 12878-12889
order: 12878
cycles: false
bibtex_author: Zhu, Zhuangdi and Hong, Junyuan and Zhou, Jiayu
author:
- given: Zhuangdi
  family: Zhu
- given: Junyuan
  family: Hong
- given: Jiayu
  family: Zhou
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zhu21b/zhu21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/zhu21b/zhu21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
