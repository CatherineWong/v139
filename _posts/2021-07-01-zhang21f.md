---
title: Bayesian Attention Belief Networks
abstract: Attention-based neural networks have achieved state-of-the-art results on
  a wide range of tasks. Most such models use deterministic attention while stochastic
  attention is less explored due to the optimization difficulties or complicated model
  design. This paper introduces Bayesian attention belief networks, which construct
  a decoder network by modeling unnormalized attention weights with a hierarchy of
  gamma distributions, and an encoder network by stacking Weibull distributions with
  a deterministic-upward-stochastic-downward structure to approximate the posterior.
  The resulting auto-encoding networks can be optimized in a differentiable way with
  a variational lower bound. It is simple to convert any models with deterministic
  attention, including pretrained ones, to the proposed Bayesian attention belief
  networks. On a variety of language understanding tasks, we show that our method
  outperforms deterministic attention and state-of-the-art stochastic attention in
  accuracy, uncertainty estimation, generalization across domains, and robustness
  to adversarial attacks. We further demonstrate the general applicability of our
  method on neural machine translation and visual question answering, showing great
  potential of incorporating our method into various attention-related tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21f
month: 0
tex_title: Bayesian Attention Belief Networks
firstpage: 12413
lastpage: 12426
page: 12413-12426
order: 12413
cycles: false
bibtex_author: Zhang, Shujian and Fan, Xinjie and Chen, Bo and Zhou, Mingyuan
author:
- given: Shujian
  family: Zhang
- given: Xinjie
  family: Fan
- given: Bo
  family: Chen
- given: Mingyuan
  family: Zhou
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zhang21f/zhang21f.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
