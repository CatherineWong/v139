---
title: Provably Efficient Learning of Transferable Rewards
abstract: The reward function is widely accepted as a succinct, robust, and transferable
  representation of a task. Typical approaches, at the basis of Inverse Reinforcement
  Learning (IRL), leverage on expert demonstrations to recover a reward function.
  In this paper, we study the theoretical properties of the class of reward functions
  that are compatible with the expert’s behavior. We analyze how the limited knowledge
  of the expert’s policy and of the environment affects the reward reconstruction
  phase. Then, we examine how the error propagates to the learned policy’s performance
  when transferring the reward function to a different environment. We employ these
  findings to devise a provably efficient active sampling approach, aware of the need
  for transferring the reward function, that can be paired with a large variety of
  IRL algorithms. Finally, we provide numerical simulations on benchmark environments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: metelli21a
month: 0
tex_title: Provably Efficient Learning of Transferable Rewards
firstpage: 7665
lastpage: 7676
page: 7665-7676
order: 7665
cycles: false
bibtex_author: Metelli, Alberto Maria and Ramponi, Giorgia and Concetti, Alessandro
  and Restelli, Marcello
author:
- given: Alberto Maria
  family: Metelli
- given: Giorgia
  family: Ramponi
- given: Alessandro
  family: Concetti
- given: Marcello
  family: Restelli
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/metelli21a/metelli21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/metelli21a/metelli21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
