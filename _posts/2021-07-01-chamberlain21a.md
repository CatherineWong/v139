---
title: 'GRAND: Graph Neural Diffusion'
abstract: We present Graph Neural Diffusion (GRAND) that approaches deep learning
  on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs)
  as discretisations of an underlying PDE. In our model, the layer structure and topology
  correspond to the discretisation choices of temporal and spatial operators. Our
  approach allows a principled development of a broad new class of GNNs that are able
  to address the common plights of graph learning models such as depth, oversmoothing,
  and bottlenecks. Key to the success of our models are stability with respect to
  perturbations in the data and this is addressed for both implicit and explicit discretisation
  schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive
  results on many standard graph benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chamberlain21a
month: 0
tex_title: 'GRAND: Graph Neural Diffusion'
firstpage: 1407
lastpage: 1418
page: 1407-1418
order: 1407
cycles: false
bibtex_author: Chamberlain, Ben and Rowbottom, James and Gorinova, Maria I and Bronstein,
  Michael and Webb, Stefan and Rossi, Emanuele
author:
- given: Ben
  family: Chamberlain
- given: James
  family: Rowbottom
- given: Maria I
  family: Gorinova
- given: Michael
  family: Bronstein
- given: Stefan
  family: Webb
- given: Emanuele
  family: Rossi
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/chamberlain21a/chamberlain21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/chamberlain21a/chamberlain21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
