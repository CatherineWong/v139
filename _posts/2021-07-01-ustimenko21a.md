---
title: 'SGLB: Stochastic Gradient Langevin Boosting'
abstract: This paper introduces Stochastic Gradient Langevin Boosting (SGLB) - a powerful
  and efficient machine learning framework that may deal with a wide range of loss
  functions and has provable generalization guarantees. The method is based on a special
  form of the Langevin diffusion equation specifically designed for gradient boosting.
  This allows us to theoretically guarantee the global convergence even for multimodal
  loss functions, while standard gradient boosting algorithms can guarantee only local
  optimum. We also empirically show that SGLB outperforms classic gradient boosting
  when applied to classification tasks with 0-1 loss function, which is known to be
  multimodal.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ustimenko21a
month: 0
tex_title: 'SGLB: Stochastic Gradient Langevin Boosting'
firstpage: 10487
lastpage: 10496
page: 10487-10496
order: 10487
cycles: false
bibtex_author: Ustimenko, Aleksei and Prokhorenkova, Liudmila
author:
- given: Aleksei
  family: Ustimenko
- given: Liudmila
  family: Prokhorenkova
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/ustimenko21a/ustimenko21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/ustimenko21a/ustimenko21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
