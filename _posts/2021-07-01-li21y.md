---
title: 'TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language
  Models'
abstract: 'Model parallelism has become a necessity for training modern large-scale
  deep language models. In this work, we identify a new and orthogonal dimension from
  existing model parallel approaches: it is possible to perform pipeline parallelism
  within a single training sequence for Transformer-based language models thanks to
  its autoregressive property. This enables a more fine-grained pipeline compared
  with previous work. With this key idea, we design TeraPipe, a high-performance token-level
  pipeline parallel algorithm for synchronous model-parallel training of Transformer-based
  language models. We develop a novel dynamic programming-based algorithm to calculate
  the optimal pipelining execution scheme given a specific model and cluster configuration.
  We show that TeraPipe can speed up the training by 5.0x for the largest GPT-3 model
  with 175 billion parameters on an AWS cluster with 48 p3.16xlarge instances compared
  with state-of-the-art model-parallel methods. The code for reproduction can be found
  at https://github.com/zhuohan123/terapipe'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li21y
month: 0
tex_title: 'TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language
  Models'
firstpage: 6543
lastpage: 6552
page: 6543-6552
order: 6543
cycles: false
bibtex_author: Li, Zhuohan and Zhuang, Siyuan and Guo, Shiyuan and Zhuo, Danyang and
  Zhang, Hao and Song, Dawn and Stoica, Ion
author:
- given: Zhuohan
  family: Li
- given: Siyuan
  family: Zhuang
- given: Shiyuan
  family: Guo
- given: Danyang
  family: Zhuo
- given: Hao
  family: Zhang
- given: Dawn
  family: Song
- given: Ion
  family: Stoica
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/li21y/li21y.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/li21y/li21y-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
