---
title: Which transformer architecture fits my data? A vocabulary bottleneck in self-attention
abstract: 'After their successful debut in natural language processing, Transformer
  architectures are now becoming the de-facto standard in many domains. An obstacle
  for their deployment over new modalities is the architectural configuration: the
  optimal depth-to-width ratio has been shown to dramatically vary across data types
  (i.e., 10x larger over images than over language). We theoretically predict the
  existence of an embedding rank bottleneck that limits the contribution of self-attention
  width to the Transformer expressivity. We thus directly tie the input vocabulary
  size and rank to the optimal depth-to-width ratio, since a small vocabulary size
  or rank dictates an added advantage of depth over width. We empirically demonstrate
  the existence of this bottleneck and its implications on the depth-to-width interplay
  of Transformer architectures, linking the architecture variability across domains
  to the often glossed-over usage of different vocabulary sizes or embedding ranks
  in different domains. As an additional benefit, our rank bottlenecking framework
  allows us to identify size redundancies of 25%-50% in leading NLP models such as
  ALBERT and T5.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wies21a
month: 0
tex_title: Which transformer architecture fits my data? A vocabulary bottleneck in
  self-attention
firstpage: 11170
lastpage: 11181
page: 11170-11181
order: 11170
cycles: false
bibtex_author: Wies, Noam and Levine, Yoav and Jannai, Daniel and Shashua, Amnon
author:
- given: Noam
  family: Wies
- given: Yoav
  family: Levine
- given: Daniel
  family: Jannai
- given: Amnon
  family: Shashua
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/wies21a/wies21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/wies21a/wies21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
