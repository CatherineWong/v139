---
title: Memory Efficient Online Meta Learning
abstract: We propose a novel algorithm for online meta learning where task instances
  are sequentially revealed with limited supervision and a learner is expected to
  meta learn them in each round, so as to allow the learner to customize a task-specific
  model rapidly with little task-level supervision. A fundamental concern arising
  in online meta-learning is the scalability of memory as more tasks are viewed over
  time. Heretofore, prior works have allowed for perfect recall leading to linear
  increase in memory with time. Different from prior works, in our method, prior task
  instances are allowed to be deleted. We propose to leverage prior task instances
  by means of a fixed-size state-vector, which is updated sequentially. Our theoretical
  analysis demonstrates that our proposed memory efficient online learning (MOML)
  method suffers sub-linear regret with convex loss functions and sub-linear local
  regret for nonconvex losses. On benchmark datasets we show that our method can outperform
  prior works even though they allow for perfect recall.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: acar21b
month: 0
tex_title: Memory Efficient Online Meta Learning
firstpage: 32
lastpage: 42
page: 32-42
order: 32
cycles: false
bibtex_author: Acar, Durmus Alp Emre and Zhu, Ruizhao and Saligrama, Venkatesh
author:
- given: Durmus Alp Emre
  family: Acar
- given: Ruizhao
  family: Zhu
- given: Venkatesh
  family: Saligrama
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/acar21b/acar21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/acar21b/acar21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
