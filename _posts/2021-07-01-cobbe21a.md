---
title: Phasic Policy Gradient
abstract: We introduce Phasic Policy Gradient (PPG), a reinforcement learning framework
  which modifies traditional on-policy actor-critic methods by separating policy and
  value function training into distinct phases. In prior methods, one must choose
  between using a shared network or separate networks to represent the policy and
  value function. Using separate networks avoids interference between objectives,
  while using a shared network allows useful features to be shared. PPG is able to
  achieve the best of both worlds by splitting optimization into two phases, one that
  advances training and one that distills features. PPG also enables the value function
  to be more aggressively optimized with a higher level of sample reuse. Compared
  to PPO, we find that PPG significantly improves sample efficiency on the challenging
  Procgen Benchmark.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cobbe21a
month: 0
tex_title: Phasic Policy Gradient
firstpage: 2020
lastpage: 2027
page: 2020-2027
order: 2020
cycles: false
bibtex_author: Cobbe, Karl W and Hilton, Jacob and Klimov, Oleg and Schulman, John
author:
- given: Karl W
  family: Cobbe
- given: Jacob
  family: Hilton
- given: Oleg
  family: Klimov
- given: John
  family: Schulman
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/cobbe21a/cobbe21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/cobbe21a/cobbe21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
