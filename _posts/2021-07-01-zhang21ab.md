---
title: 'Model-Free Reinforcement Learning: from Clipped Pseudo-Regret to Sample Complexity'
abstract: In this paper we consider the problem of learning an $\epsilon$-optimal
  policy for a discounted Markov Decision Process (MDP). Given an MDP with $S$ states,
  $A$ actions, the discount factor $\gamma \in (0,1)$, and an approximation threshold
  $\epsilon > 0$, we provide a model-free algorithm to learn an $\epsilon$-optimal
  policy with sample complexity $\tilde{O}(\frac{SA\ln(1/p)}{\epsilon^2(1-\gamma)^{5.5}})$
  \footnote{In this work, the notation $\tilde{O}(\cdot)$ hides poly-logarithmic factors
  of $S,A,1/(1-\gamma)$, and $1/\epsilon$.} and success probability $(1-p)$. For small
  enough $\epsilon$, we show an improved algorithm with sample complexity $\tilde{O}(\frac{SA\ln(1/p)}{\epsilon^2(1-\gamma)^{3}})$.
  While the first bound improves upon all known model-free algorithms and model-based
  ones with tight dependence on $S$, our second algorithm beats all known sample complexity
  bounds and matches the information theoretic lower bound up to logarithmic factors.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21ab
month: 0
tex_title: 'Model-Free Reinforcement Learning: from Clipped Pseudo-Regret to Sample
  Complexity'
firstpage: 12653
lastpage: 12662
page: 12653-12662
order: 12653
cycles: false
bibtex_author: Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang
author:
- given: Zihan
  family: Zhang
- given: Yuan
  family: Zhou
- given: Xiangyang
  family: Ji
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zhang21ab/zhang21ab.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/zhang21ab/zhang21ab-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
