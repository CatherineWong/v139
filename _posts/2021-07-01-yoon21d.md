---
title: Accelerated Algorithms for Smooth Convex-Concave Minimax Problems with O(1/k^2)
  Rate on Squared Gradient Norm
abstract: In this work, we study the computational complexity of reducing the squared
  gradient magnitude for smooth minimax optimization problems. First, we present algorithms
  with accelerated $\mathcal{O}(1/k^2)$ last-iterate rates, faster than the existing
  $\mathcal{O}(1/k)$ or slower rates for extragradient, Popov, and gradient descent
  with anchoring. The acceleration mechanism combines extragradient steps with anchoring
  and is distinct from Nesterovâ€™s acceleration. We then establish optimality of the
  $\mathcal{O}(1/k^2)$ rate through a matching lower bound.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yoon21d
month: 0
tex_title: Accelerated Algorithms for Smooth Convex-Concave Minimax Problems with
  O(1/k^2) Rate on Squared Gradient Norm
firstpage: 12098
lastpage: 12109
page: 12098-12109
order: 12098
cycles: false
bibtex_author: Yoon, Taeho and Ryu, Ernest K
author:
- given: Taeho
  family: Yoon
- given: Ernest K
  family: Ryu
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/yoon21d/yoon21d.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/yoon21d/yoon21d-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
