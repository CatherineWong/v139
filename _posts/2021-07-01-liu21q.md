---
title: Temporal Difference Learning as Gradient Splitting
abstract: Temporal difference learning with linear function approximation is a popular
  method to obtain a low-dimensional approximation of the value function of a policy
  in a Markov Decision Process. We provide an interpretation of this method in terms
  of a splitting of the gradient of an appropriately chosen function. As a consequence
  of this interpretation, convergence proofs for gradient descent can be applied almost
  verbatim to temporal difference learning. Beyond giving a fuller explanation of
  why temporal difference works, this interpretation also yields improved convergence
  times. We consider the setting with $1/\sqrt{T}$ step-size, where previous comparable
  finite-time convergence time bounds for temporal difference learning had the multiplicative
  factor $1/(1-\gamma)$ in front of the bound, with $\gamma$ being the discount factor.
  We show that a minor variation on TD learning which estimates the mean of the value
  function separately has a convergence time where $1/(1-\gamma)$ only multiplies
  an asymptotically negligible term.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu21q
month: 0
tex_title: Temporal Difference Learning as Gradient Splitting
firstpage: 6905
lastpage: 6913
page: 6905-6913
order: 6905
cycles: false
bibtex_author: Liu, Rui and Olshevsky, Alex
author:
- given: Rui
  family: Liu
- given: Alex
  family: Olshevsky
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/liu21q/liu21q.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/liu21q/liu21q-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
