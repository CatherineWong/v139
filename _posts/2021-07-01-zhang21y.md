---
title: Breaking the Deadly Triad with a Target Network
abstract: The deadly triad refers to the instability of a reinforcement learning algorithm
  when it employs off-policy learning, function approximation, and bootstrapping simultaneously.
  In this paper, we investigate the target network as a tool for breaking the deadly
  triad, providing theoretical support for the conventional wisdom that a target network
  stabilizes training. We first propose and analyze a novel target network update
  rule which augments the commonly used Polyak-averaging style update with two projections.
  We then apply the target network and ridge regularization in several divergent algorithms
  and show their convergence to regularized TD fixed points. Those algorithms are
  off-policy with linear function approximation and bootstrapping, spanning both policy
  evaluation and control, as well as both discounted and average-reward settings.
  In particular, we provide the first convergent linear $Q$-learning algorithms under
  nonrestrictive and changing behavior policies without bi-level optimization.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21y
month: 0
tex_title: Breaking the Deadly Triad with a Target Network
firstpage: 12621
lastpage: 12631
page: 12621-12631
order: 12621
cycles: false
bibtex_author: Zhang, Shangtong and Yao, Hengshuai and Whiteson, Shimon
author:
- given: Shangtong
  family: Zhang
- given: Hengshuai
  family: Yao
- given: Shimon
  family: Whiteson
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/zhang21y/zhang21y.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/zhang21y/zhang21y-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
