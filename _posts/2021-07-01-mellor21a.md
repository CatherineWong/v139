---
title: Neural Architecture Search without Training
abstract: The time and effort involved in hand-designing deep neural networks is immense.
  This has prompted the development of Neural Architecture Search (NAS) techniques
  to automate this design. However, NAS algorithms tend to be slow and expensive;
  they need to train vast numbers of candidate networks to inform the search process.
  This could be alleviated if we could partially predict a network’s trained accuracy
  from its initial state. In this work, we examine the overlap of activations between
  datapoints in untrained networks and motivate how this can give a measure which
  is usefully indicative of a network’s trained performance. We incorporate this measure
  into a simple algorithm that allows us to search for powerful networks without any
  training in a matter of seconds on a single GPU, and verify its effectiveness on
  NAS-Bench-101, NAS-Bench-201, NATS-Bench, and Network Design Spaces. Our approach
  can be readily combined with more expensive search methods; we examine a simple
  adaptation of regularised evolutionary search. Code for reproducing our experiments
  is available at https://github.com/BayesWatch/nas-without-training.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mellor21a
month: 0
tex_title: Neural Architecture Search without Training
firstpage: 7588
lastpage: 7598
page: 7588-7598
order: 7588
cycles: false
bibtex_author: Mellor, Joe and Turner, Jack and Storkey, Amos and Crowley, Elliot
  J
author:
- given: Joe
  family: Mellor
- given: Jack
  family: Turner
- given: Amos
  family: Storkey
- given: Elliot J
  family: Crowley
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/mellor21a/mellor21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/mellor21a/mellor21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
