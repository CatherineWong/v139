---
title: 'Catformer: Designing Stable Transformers via Sensitivity Analysis'
abstract: Transformer architectures are widely used, but training them is non-trivial,
  requiring custom learning rate schedules, scaling terms, residual connections, careful
  placement of submodules such as normalization, and so on. In this paper, we improve
  upon recent analysis of Transformers and formalize a notion of sensitivity to capture
  the difficulty of training. Sensitivity characterizes how the variance of activation
  and gradient norms change in expectation when parameters are randomly perturbed.
  We analyze the sensitivity of previous Transformer architectures and design a new
  architecture, the Catformer, which replaces residual connections or RNN-based gating
  mechanisms with concatenation. We prove that Catformers are less sensitive than
  other Transformer variants and demonstrate that this leads to more stable training.
  On DMLab30, a suite of high-dimension reinforcement tasks, Catformer outperforms
  other transformers, including Gated Transformer-XL—the state-of-the-art architecture
  designed to address stability—by 13%.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: davis21a
month: 0
tex_title: 'Catformer: Designing Stable Transformers via Sensitivity Analysis'
firstpage: 2489
lastpage: 2499
page: 2489-2499
order: 2489
cycles: false
bibtex_author: Davis, Jared Q and Gu, Albert and Choromanski, Krzysztof and Dao, Tri
  and Re, Christopher and Finn, Chelsea and Liang, Percy
author:
- given: Jared Q
  family: Davis
- given: Albert
  family: Gu
- given: Krzysztof
  family: Choromanski
- given: Tri
  family: Dao
- given: Christopher
  family: Re
- given: Chelsea
  family: Finn
- given: Percy
  family: Liang
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/davis21a/davis21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/davis21a/davis21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
