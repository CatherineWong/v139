---
title: Provable Generalization of SGD-trained Neural Networks of Any Width in the
  Presence of Adversarial Label Noise
abstract: We consider a one-hidden-layer leaky ReLU network of arbitrary width trained
  by stochastic gradient descent (SGD) following an arbitrary initialization. We prove
  that SGD produces neural networks that have classification accuracy competitive
  with that of the best halfspace over the distribution for a broad class of distributions
  that includes log-concave isotropic and hard margin distributions. Equivalently,
  such networks can generalize when the data distribution is linearly separable but
  corrupted with adversarial label noise, despite the capacity to overfit. To the
  best of our knowledge, this is the first work to show that overparameterized neural
  networks trained by SGD can generalize when the data is corrupted with adversarial
  label noise.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: frei21b
month: 0
tex_title: Provable Generalization of SGD-trained Neural Networks of Any Width in
  the Presence of Adversarial Label Noise
firstpage: 3427
lastpage: 3438
page: 3427-3438
order: 3427
cycles: false
bibtex_author: Frei, Spencer and Cao, Yuan and Gu, Quanquan
author:
- given: Spencer
  family: Frei
- given: Yuan
  family: Cao
- given: Quanquan
  family: Gu
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/frei21b/frei21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/frei21b/frei21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
