---
title: Acceleration via Fractal Learning Rate Schedules
abstract: 'In practical applications of iterative first-order optimization, the learning
  rate schedule remains notoriously difficult to understand and expensive to tune.
  We demonstrate the presence of these subtleties even in the innocuous case when
  the objective is a convex quadratic. We reinterpret an iterative algorithm from
  the numerical analysis literature as what we call the Chebyshev learning rate schedule
  for accelerating vanilla gradient descent, and show that the problem of mitigating
  instability leads to a fractal ordering of step sizes. We provide some experiments
  to challenge conventional beliefs about stable learning rates in deep learning:
  the fractal schedule enables training to converge with locally unstable updates
  which make negative progress on the objective.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: agarwal21a
month: 0
tex_title: Acceleration via Fractal Learning Rate Schedules
firstpage: 87
lastpage: 99
page: 87-99
order: 87
cycles: false
bibtex_author: Agarwal, Naman and Goel, Surbhi and Zhang, Cyril
author:
- given: Naman
  family: Agarwal
- given: Surbhi
  family: Goel
- given: Cyril
  family: Zhang
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/agarwal21a/agarwal21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/agarwal21a/agarwal21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
