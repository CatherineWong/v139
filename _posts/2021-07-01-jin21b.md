---
title: Towards Tight Bounds on the Sample Complexity of Average-reward MDPs
abstract: We prove new upper and lower bounds for sample complexity of finding an
  $\epsilon$-optimal policy of an infinite-horizon average-reward Markov decision
  process (MDP) given access to a generative model. When the mixing time of the probability
  transition matrix of all policies is at most $t_\mathrm{mix}$, we provide an algorithm
  that solves the problem using $\widetilde{O}(t_\mathrm{mix} \epsilon^{-3})$ (oblivious)
  samples per state-action pair. Further, we provide a lower bound showing that a
  linear dependence on $t_\mathrm{mix}$ is necessary in the worst case for any algorithm
  which computes oblivious samples. We obtain our results by establishing connections
  between infinite-horizon average-reward MDPs and discounted MDPs of possible further
  utility.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jin21b
month: 0
tex_title: Towards Tight Bounds on the Sample Complexity of Average-reward MDPs
firstpage: 5055
lastpage: 5064
page: 5055-5064
order: 5055
cycles: false
bibtex_author: Jin, Yujia and Sidford, Aaron
author:
- given: Yujia
  family: Jin
- given: Aaron
  family: Sidford
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/jin21b/jin21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/jin21b/jin21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
